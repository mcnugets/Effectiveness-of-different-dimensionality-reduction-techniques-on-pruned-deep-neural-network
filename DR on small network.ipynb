{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "linear_relu_stack.0.weight torch.Size([512, 784])\n",
      "linear_relu_stack.0.bias torch.Size([512])\n",
      "linear_relu_stack.2.weight torch.Size([512, 512])\n",
      "linear_relu_stack.2.bias torch.Size([512])\n",
      "linear_relu_stack.4.weight torch.Size([10, 512])\n",
      "linear_relu_stack.4.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tensor tensor([[[4.2696e-01, 1.4153e-01, 7.5251e-01, 2.9177e-01, 5.5462e-01,\n",
      "          6.8289e-01, 3.7364e-01, 1.2884e-01, 3.9452e-01, 3.2259e-01,\n",
      "          3.3647e-01, 6.3034e-02, 6.0368e-01, 2.2058e-03, 8.3479e-01,\n",
      "          6.2475e-01, 3.9060e-01, 6.9036e-01, 5.8445e-01, 6.4414e-01,\n",
      "          7.4710e-01, 7.4345e-01, 4.4199e-01, 4.3336e-01, 4.6245e-01,\n",
      "          7.0071e-01, 8.5687e-01, 5.7807e-01],\n",
      "         [2.7409e-01, 7.5895e-01, 4.1701e-01, 3.1125e-01, 6.6099e-01,\n",
      "          5.0300e-01, 5.1150e-01, 4.0186e-01, 4.5110e-01, 4.3538e-02,\n",
      "          9.6169e-01, 5.9079e-01, 2.7195e-01, 1.9811e-01, 8.1633e-01,\n",
      "          8.7977e-01, 1.1034e-01, 7.4785e-01, 9.2868e-01, 4.3496e-01,\n",
      "          5.5518e-01, 3.6045e-01, 8.8264e-01, 8.1049e-01, 3.6671e-02,\n",
      "          7.6521e-02, 1.7983e-01, 4.4812e-01],\n",
      "         [3.7915e-01, 5.6466e-01, 4.4387e-02, 2.4941e-01, 7.7652e-01,\n",
      "          2.1149e-01, 3.1657e-01, 3.9956e-01, 1.7366e-01, 8.5122e-01,\n",
      "          3.1438e-01, 3.9172e-01, 6.8974e-01, 3.0559e-01, 6.1865e-01,\n",
      "          8.4340e-01, 3.5713e-01, 2.3997e-01, 1.5772e-01, 4.6777e-01,\n",
      "          2.5804e-01, 8.1696e-02, 1.9649e-01, 5.9208e-02, 6.3269e-01,\n",
      "          1.4696e-01, 5.8539e-01, 6.3473e-01],\n",
      "         [9.7781e-01, 4.2575e-01, 2.5040e-01, 7.8893e-01, 7.6608e-01,\n",
      "          4.3633e-01, 2.8553e-01, 6.7636e-01, 4.2448e-01, 1.7166e-01,\n",
      "          5.3681e-01, 4.4701e-01, 5.1039e-01, 6.0938e-01, 3.2951e-01,\n",
      "          9.2440e-01, 1.5477e-01, 2.7459e-01, 9.1171e-01, 5.6787e-01,\n",
      "          4.2403e-01, 8.2084e-01, 5.9228e-01, 7.4543e-01, 6.7522e-02,\n",
      "          9.0549e-01, 4.5215e-01, 4.0220e-01],\n",
      "         [1.0126e-01, 4.9302e-01, 4.8425e-01, 2.1841e-01, 6.1716e-01,\n",
      "          4.8780e-01, 7.9300e-02, 1.9327e-02, 3.8261e-02, 3.4251e-01,\n",
      "          5.3603e-01, 6.3399e-01, 6.6252e-01, 1.0761e-01, 9.4064e-01,\n",
      "          1.7199e-01, 6.4234e-01, 6.0291e-01, 1.8970e-01, 7.2399e-01,\n",
      "          4.4402e-01, 8.7599e-01, 7.0203e-01, 7.7889e-01, 2.1235e-01,\n",
      "          3.6744e-01, 7.8086e-01, 2.6515e-01],\n",
      "         [6.0020e-01, 2.9977e-01, 6.9596e-01, 5.3091e-01, 9.0716e-02,\n",
      "          5.1483e-01, 9.4015e-01, 3.6298e-01, 9.1335e-01, 5.5481e-01,\n",
      "          2.8112e-01, 8.3661e-01, 3.4320e-01, 9.6143e-02, 5.2652e-01,\n",
      "          9.9391e-01, 6.8478e-01, 1.9262e-01, 5.5456e-01, 5.5360e-01,\n",
      "          6.6113e-01, 1.2527e-01, 3.8079e-01, 1.6066e-01, 8.1262e-01,\n",
      "          7.2278e-01, 9.9160e-01, 1.1029e-01],\n",
      "         [8.8410e-01, 8.7180e-01, 2.5470e-01, 2.4410e-01, 8.0127e-01,\n",
      "          4.3526e-01, 4.1076e-01, 7.6728e-01, 9.8557e-01, 7.5398e-01,\n",
      "          2.4454e-01, 6.4903e-01, 1.7750e-01, 1.1415e-01, 1.1304e-01,\n",
      "          3.0472e-01, 3.3587e-01, 6.8048e-01, 4.2466e-01, 1.6326e-01,\n",
      "          5.8320e-01, 7.9899e-01, 8.6922e-01, 1.0026e-01, 3.7418e-01,\n",
      "          2.6473e-01, 8.6465e-02, 7.4172e-01],\n",
      "         [6.1499e-01, 2.6795e-02, 5.6542e-01, 1.5266e-01, 9.4822e-01,\n",
      "          7.8413e-01, 8.8554e-01, 6.1151e-01, 8.0837e-01, 5.4515e-01,\n",
      "          7.8196e-01, 1.6923e-01, 2.8788e-01, 6.1268e-01, 2.1143e-01,\n",
      "          3.4012e-01, 4.9227e-01, 2.2079e-01, 7.0453e-01, 5.7451e-01,\n",
      "          7.8462e-01, 6.4994e-02, 1.9780e-01, 3.8304e-01, 8.1888e-01,\n",
      "          2.7623e-01, 1.3323e-01, 7.6408e-01],\n",
      "         [8.5497e-01, 5.4973e-01, 6.9196e-01, 4.8922e-01, 3.5220e-01,\n",
      "          5.2371e-01, 8.8431e-01, 1.8812e-01, 4.3318e-01, 8.3223e-01,\n",
      "          3.3210e-01, 6.5471e-02, 9.7735e-01, 8.0995e-01, 7.1811e-01,\n",
      "          5.9526e-01, 7.0992e-02, 5.1912e-01, 5.3158e-01, 5.6915e-01,\n",
      "          7.8004e-01, 3.9745e-01, 5.5865e-02, 9.8351e-01, 1.3914e-02,\n",
      "          6.7006e-01, 2.2530e-01, 1.4706e-01],\n",
      "         [7.3449e-01, 1.5414e-01, 4.4589e-02, 5.2168e-01, 9.9408e-01,\n",
      "          6.6007e-01, 4.1716e-01, 5.1694e-02, 7.9852e-01, 7.3925e-01,\n",
      "          2.7474e-02, 4.3448e-01, 4.9614e-01, 9.8399e-01, 6.9471e-01,\n",
      "          9.0048e-01, 4.2034e-01, 3.4988e-01, 8.4529e-01, 9.9457e-01,\n",
      "          1.9332e-01, 1.5848e-01, 2.5038e-02, 9.9977e-01, 2.9354e-01,\n",
      "          8.1136e-01, 2.7860e-01, 7.2090e-01],\n",
      "         [7.1922e-01, 6.4771e-01, 6.7742e-01, 3.7506e-01, 4.6099e-01,\n",
      "          6.1745e-01, 8.7017e-01, 6.8037e-01, 1.9410e-02, 8.6856e-01,\n",
      "          4.6570e-02, 6.8797e-01, 8.5464e-01, 4.3440e-01, 4.9676e-01,\n",
      "          2.7504e-01, 1.1749e-01, 6.2361e-01, 7.0952e-02, 4.8066e-01,\n",
      "          8.7032e-01, 7.7383e-01, 3.7259e-01, 1.1445e-01, 9.4850e-01,\n",
      "          5.2451e-01, 3.9109e-01, 2.7295e-01],\n",
      "         [7.9474e-01, 2.8937e-01, 2.5198e-01, 4.6989e-01, 9.0303e-02,\n",
      "          2.5312e-01, 8.6366e-01, 4.2310e-01, 8.4264e-01, 5.4151e-01,\n",
      "          2.8866e-01, 9.4730e-01, 3.4881e-01, 2.0664e-01, 8.0130e-01,\n",
      "          9.9746e-01, 9.0250e-01, 8.7653e-01, 7.2685e-02, 7.1172e-01,\n",
      "          9.4837e-02, 1.0227e-01, 4.2211e-01, 5.1143e-01, 5.8900e-01,\n",
      "          8.6255e-01, 9.4132e-01, 4.0987e-01],\n",
      "         [4.1631e-01, 1.7575e-01, 4.0076e-02, 8.3845e-01, 2.7353e-01,\n",
      "          5.9770e-01, 6.6205e-01, 3.0268e-01, 9.0166e-01, 7.2355e-02,\n",
      "          5.6981e-01, 3.7979e-01, 4.0594e-01, 1.6443e-02, 8.7779e-01,\n",
      "          9.8056e-01, 3.6686e-01, 2.3414e-01, 9.0329e-01, 7.8859e-01,\n",
      "          1.3856e-01, 5.6604e-01, 4.4621e-01, 4.4182e-01, 4.2526e-01,\n",
      "          7.0003e-01, 6.7433e-01, 2.9852e-01],\n",
      "         [6.9672e-01, 6.8531e-01, 5.6511e-01, 8.6838e-03, 7.4084e-01,\n",
      "          4.7584e-01, 5.2313e-01, 1.4584e-01, 6.5910e-01, 5.8708e-01,\n",
      "          7.6324e-01, 7.4015e-01, 6.8512e-01, 3.7138e-01, 9.5912e-01,\n",
      "          7.3226e-01, 5.2969e-01, 7.1775e-01, 1.6540e-01, 7.5679e-01,\n",
      "          3.1908e-01, 6.2272e-01, 7.2741e-01, 4.8611e-01, 8.0174e-01,\n",
      "          6.3648e-02, 8.0097e-02, 2.1340e-01],\n",
      "         [5.6015e-01, 9.5403e-01, 9.2673e-02, 5.6214e-01, 6.8905e-01,\n",
      "          8.0981e-01, 5.0224e-01, 3.9151e-01, 8.3019e-01, 2.0306e-01,\n",
      "          2.4486e-01, 1.2425e-01, 3.3553e-01, 2.1797e-01, 8.3881e-01,\n",
      "          7.4232e-01, 5.3083e-01, 8.5957e-01, 8.5913e-01, 5.4054e-01,\n",
      "          4.7408e-01, 5.4028e-01, 1.4080e-01, 5.0981e-01, 7.4683e-01,\n",
      "          7.7400e-01, 8.1427e-01, 3.6091e-01],\n",
      "         [5.4454e-01, 2.6684e-01, 4.7956e-01, 5.3542e-01, 5.4662e-01,\n",
      "          3.2333e-01, 8.3802e-01, 2.5272e-02, 9.1057e-01, 3.5454e-01,\n",
      "          9.0014e-01, 1.5520e-01, 1.7144e-01, 2.7530e-01, 8.4012e-01,\n",
      "          7.7676e-01, 5.8211e-01, 1.3878e-01, 1.7168e-01, 5.0436e-01,\n",
      "          3.0426e-01, 8.6868e-01, 2.8180e-01, 4.0096e-01, 9.6698e-01,\n",
      "          6.8740e-01, 7.6522e-01, 8.5057e-01],\n",
      "         [7.1129e-02, 8.4517e-01, 2.1392e-01, 8.0842e-01, 2.8474e-01,\n",
      "          3.8287e-01, 5.1209e-01, 4.3223e-01, 9.4276e-01, 5.6242e-01,\n",
      "          1.4297e-01, 8.9087e-01, 4.5444e-01, 4.1274e-01, 8.3317e-02,\n",
      "          7.9606e-02, 8.1605e-01, 6.3926e-01, 8.1354e-01, 9.7117e-01,\n",
      "          2.0022e-02, 9.8335e-01, 4.5931e-01, 9.6770e-01, 4.4081e-01,\n",
      "          6.4229e-01, 2.5017e-01, 5.9712e-01],\n",
      "         [4.6872e-01, 5.4884e-01, 2.7430e-01, 4.2103e-01, 1.4691e-01,\n",
      "          3.5659e-01, 6.2070e-01, 3.6641e-01, 2.3901e-04, 1.9714e-01,\n",
      "          4.5351e-01, 4.2951e-01, 1.6415e-01, 1.7184e-02, 1.3704e-01,\n",
      "          6.1604e-01, 8.0411e-01, 3.4996e-01, 5.7353e-01, 1.2790e-01,\n",
      "          4.3493e-01, 3.2852e-01, 5.0385e-01, 4.5170e-01, 3.8760e-01,\n",
      "          4.2437e-01, 5.3587e-01, 3.2490e-01],\n",
      "         [3.2973e-01, 4.1909e-01, 2.8453e-01, 6.7484e-01, 2.7677e-01,\n",
      "          6.6434e-01, 4.2707e-01, 4.8240e-01, 3.5335e-01, 9.1106e-01,\n",
      "          2.2652e-02, 2.3998e-01, 1.7488e-01, 1.0819e-01, 3.1778e-01,\n",
      "          8.9141e-01, 6.0054e-01, 9.4618e-02, 4.2038e-01, 3.3347e-01,\n",
      "          2.3498e-02, 9.0384e-01, 9.9817e-01, 4.1685e-01, 7.6105e-01,\n",
      "          4.5860e-01, 7.3628e-02, 6.7712e-01],\n",
      "         [7.1720e-01, 1.9634e-01, 7.7096e-01, 2.5962e-01, 6.5475e-01,\n",
      "          4.8616e-01, 3.7090e-01, 8.0736e-01, 7.0275e-01, 5.0327e-01,\n",
      "          5.3138e-01, 1.0871e-01, 5.7091e-01, 8.6322e-01, 9.2061e-03,\n",
      "          5.1839e-01, 6.5063e-01, 3.4558e-01, 3.2251e-01, 9.2965e-01,\n",
      "          5.3319e-01, 4.7118e-01, 6.2928e-01, 2.7545e-03, 8.1041e-01,\n",
      "          4.5696e-01, 5.6353e-01, 8.1116e-01],\n",
      "         [7.5319e-01, 3.8239e-01, 8.4643e-01, 2.6295e-01, 5.9911e-01,\n",
      "          4.8725e-01, 8.6466e-01, 3.2572e-01, 8.1579e-01, 2.1932e-01,\n",
      "          5.9521e-01, 1.5879e-01, 7.2394e-01, 7.4056e-01, 8.6317e-01,\n",
      "          3.9022e-01, 8.0646e-01, 1.7995e-01, 6.7441e-01, 2.8553e-01,\n",
      "          6.6704e-02, 4.9157e-01, 7.1832e-01, 3.8824e-01, 6.3971e-01,\n",
      "          9.0959e-01, 6.1620e-01, 5.3507e-01],\n",
      "         [8.4958e-01, 4.7846e-01, 8.1095e-01, 7.8355e-02, 2.3688e-01,\n",
      "          5.4834e-01, 2.5202e-01, 7.3885e-01, 2.9559e-01, 2.4080e-01,\n",
      "          2.5659e-02, 5.1698e-01, 6.6033e-01, 8.0343e-01, 2.2862e-01,\n",
      "          3.4092e-01, 7.9247e-01, 5.2537e-01, 9.8812e-01, 3.3779e-01,\n",
      "          5.4832e-01, 2.5720e-01, 5.2709e-01, 1.4476e-01, 8.8339e-01,\n",
      "          1.0816e-01, 2.0650e-01, 1.8396e-01],\n",
      "         [6.3728e-01, 6.0534e-02, 6.2409e-01, 8.2505e-01, 3.5009e-01,\n",
      "          3.0048e-01, 7.2835e-02, 2.7714e-01, 1.7988e-01, 3.9361e-03,\n",
      "          3.2919e-01, 6.8060e-01, 6.3055e-01, 8.5051e-01, 9.9508e-01,\n",
      "          6.4786e-02, 4.0129e-01, 6.2483e-01, 4.4592e-01, 7.9158e-01,\n",
      "          2.0915e-03, 9.6595e-01, 2.2607e-01, 1.1131e-01, 2.5572e-01,\n",
      "          4.7238e-01, 5.7753e-01, 4.0752e-01],\n",
      "         [5.3030e-01, 1.5381e-02, 6.3859e-01, 2.4974e-01, 9.9569e-01,\n",
      "          5.8778e-01, 4.5853e-02, 2.4372e-01, 1.4780e-01, 1.4001e-01,\n",
      "          1.7222e-01, 9.0259e-02, 3.7516e-01, 9.4583e-01, 2.6555e-01,\n",
      "          7.3770e-01, 3.1448e-01, 7.9916e-01, 1.3058e-01, 4.0433e-01,\n",
      "          4.0262e-01, 3.2430e-01, 6.6306e-01, 2.5038e-01, 4.4350e-01,\n",
      "          4.4595e-01, 4.9935e-01, 1.7143e-01],\n",
      "         [5.4440e-01, 2.5486e-01, 3.1844e-01, 8.4719e-02, 6.9534e-01,\n",
      "          8.8824e-01, 4.1383e-02, 9.4722e-01, 1.9748e-01, 2.0460e-01,\n",
      "          8.4754e-01, 5.5159e-01, 9.6041e-01, 5.0235e-01, 4.4388e-01,\n",
      "          7.1523e-02, 6.9527e-01, 7.4438e-01, 4.7424e-01, 8.8829e-01,\n",
      "          7.8559e-01, 3.4359e-01, 9.8076e-01, 7.3302e-01, 9.7611e-03,\n",
      "          1.3995e-01, 6.6277e-01, 2.0821e-01],\n",
      "         [4.1700e-01, 4.5637e-01, 4.4372e-01, 1.2641e-01, 2.4279e-01,\n",
      "          5.2574e-01, 1.2925e-01, 6.4137e-01, 3.2483e-01, 8.6876e-01,\n",
      "          5.7328e-01, 4.8764e-01, 5.1674e-01, 9.8518e-01, 4.1702e-02,\n",
      "          7.7584e-01, 5.4383e-01, 4.2499e-01, 3.0971e-01, 2.7429e-01,\n",
      "          5.8982e-01, 1.7373e-02, 9.8633e-01, 4.4342e-01, 8.6530e-01,\n",
      "          3.8700e-02, 5.2538e-01, 1.5586e-01],\n",
      "         [2.0607e-01, 4.2091e-01, 9.9438e-01, 6.0468e-01, 3.6734e-01,\n",
      "          3.2985e-01, 3.8155e-01, 1.2563e-01, 3.5560e-01, 2.2531e-01,\n",
      "          9.0731e-01, 3.6447e-01, 2.1040e-01, 7.8446e-01, 9.2777e-01,\n",
      "          3.0308e-01, 2.5395e-01, 4.0445e-01, 2.9577e-01, 6.3867e-01,\n",
      "          6.5289e-02, 9.2513e-01, 8.1735e-01, 8.0801e-01, 9.4344e-01,\n",
      "          2.9977e-02, 9.9024e-02, 2.1990e-01],\n",
      "         [5.2945e-01, 5.4986e-01, 1.0310e-01, 8.7955e-01, 6.9474e-01,\n",
      "          7.1639e-01, 6.8622e-01, 3.0926e-01, 3.5411e-01, 8.5156e-02,\n",
      "          8.7313e-01, 6.8454e-01, 2.9090e-01, 3.3427e-01, 4.8578e-02,\n",
      "          1.4058e-01, 8.7054e-01, 3.6075e-01, 4.9848e-01, 6.1212e-01,\n",
      "          2.5587e-01, 5.7235e-01, 7.3429e-01, 3.7652e-01, 3.5826e-01,\n",
      "          3.5404e-01, 8.7736e-01, 7.8873e-01]]])\n",
      "pred probability_ tensor([[0.0913, 0.0944, 0.0966, 0.0981, 0.1004, 0.1063, 0.1085, 0.0998, 0.1053,\n",
      "         0.0994]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28, device=device)\n",
    "print('the tensor', X)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print('pred probability_', pred_probab)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "new_image = torch.rand(3,28,28)\n",
    "print(new_image.shape)\n",
    "new_image_flat = nn.Flatten()(new_image)\n",
    "print(new_image_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n",
      "tensor([[-0.0431,  0.2953,  0.2094, -0.2261,  0.1049,  0.7387,  0.1223, -0.0431,\n",
      "          0.0526,  0.0228,  0.2656, -0.1338,  0.1667, -0.1056,  0.5041,  0.3411,\n",
      "         -0.3468, -0.0540,  0.3019, -0.0029],\n",
      "        [-0.0167,  0.1232, -0.1887,  0.1269,  0.3108,  0.4942, -0.0067,  0.1927,\n",
      "          0.1814,  0.2870, -0.0220, -0.0643,  0.2641, -0.3040,  0.4749,  0.2696,\n",
      "         -0.1378,  0.0864,  0.2127,  0.1048],\n",
      "        [ 0.1234,  0.5102,  0.3264,  0.2336,  0.1470,  0.7030, -0.0779, -0.2489,\n",
      "          0.2600, -0.0125,  0.1482, -0.6005, -0.0117, -0.4549,  0.6568, -0.1463,\n",
      "         -0.2320,  0.2022,  0.0350, -0.3296]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(new_image_flat)\n",
    "print(hidden1.size())\n",
    "print(hidden1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.0431,  0.2953,  0.2094, -0.2261,  0.1049,  0.7387,  0.1223, -0.0431,\n",
      "          0.0526,  0.0228,  0.2656, -0.1338,  0.1667, -0.1056,  0.5041,  0.3411,\n",
      "         -0.3468, -0.0540,  0.3019, -0.0029],\n",
      "        [-0.0167,  0.1232, -0.1887,  0.1269,  0.3108,  0.4942, -0.0067,  0.1927,\n",
      "          0.1814,  0.2870, -0.0220, -0.0643,  0.2641, -0.3040,  0.4749,  0.2696,\n",
      "         -0.1378,  0.0864,  0.2127,  0.1048],\n",
      "        [ 0.1234,  0.5102,  0.3264,  0.2336,  0.1470,  0.7030, -0.0779, -0.2489,\n",
      "          0.2600, -0.0125,  0.1482, -0.6005, -0.0117, -0.4549,  0.6568, -0.1463,\n",
      "         -0.2320,  0.2022,  0.0350, -0.3296]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.0000, 0.2953, 0.2094, 0.0000, 0.1049, 0.7387, 0.1223, 0.0000, 0.0526,\n",
      "         0.0228, 0.2656, 0.0000, 0.1667, 0.0000, 0.5041, 0.3411, 0.0000, 0.0000,\n",
      "         0.3019, 0.0000],\n",
      "        [0.0000, 0.1232, 0.0000, 0.1269, 0.3108, 0.4942, 0.0000, 0.1927, 0.1814,\n",
      "         0.2870, 0.0000, 0.0000, 0.2641, 0.0000, 0.4749, 0.2696, 0.0000, 0.0864,\n",
      "         0.2127, 0.1048],\n",
      "        [0.1234, 0.5102, 0.3264, 0.2336, 0.1470, 0.7030, 0.0000, 0.0000, 0.2600,\n",
      "         0.0000, 0.1482, 0.0000, 0.0000, 0.0000, 0.6568, 0.0000, 0.0000, 0.2022,\n",
      "         0.0350, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([3, 10])\n",
      "tensor output:  tensor([[ 0.0310,  0.0588, -0.0087,  0.0181,  0.0285,  0.0176,  0.0396,  0.0557,\n",
      "         -0.0008, -0.0584],\n",
      "        [ 0.0332,  0.1023,  0.0550,  0.0579,  0.0206, -0.0187,  0.0371,  0.0556,\n",
      "         -0.0295, -0.0864],\n",
      "        [ 0.0318,  0.0441,  0.0232,  0.0444,  0.0319, -0.0053,  0.0179,  0.0415,\n",
      "          0.0287, -0.0619]], grad_fn=<AddmmBackward0>)\n",
      "torch.Size([3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28*28, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "input_image2 = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image2)\n",
    "\n",
    "print('shape', logits.shape)\n",
    "\n",
    "\n",
    "print('tensor output: ', logits)\n",
    "\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "pred_probab = softmax(logits)\n",
    "\n",
    "print(pred_probab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Layer: 1.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0307,  0.0080,  0.0082,  ...,  0.0229,  0.0047, -0.0140],\n",
      "        [ 0.0166,  0.0079,  0.0144,  ...,  0.0328,  0.0299,  0.0040]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 1.bias | Size: torch.Size([512]) | Values : tensor([ 0.0039, -0.0316], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 3.weight | Size: torch.Size([256, 512]) | Values : tensor([[ 0.0220,  0.0403, -0.0094,  ...,  0.0276,  0.0143, -0.0341],\n",
      "        [ 0.0140,  0.0297,  0.0095,  ...,  0.0387,  0.0167,  0.0424]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 3.bias | Size: torch.Size([256]) | Values : tensor([-0.0180, -0.0154], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 5.weight | Size: torch.Size([10, 256]) | Values : tensor([[-0.0271,  0.0250,  0.0060, -0.0565, -0.0013,  0.0497,  0.0387,  0.0199,\n",
      "         -0.0463,  0.0573, -0.0559, -0.0069, -0.0474,  0.0278, -0.0211,  0.0484,\n",
      "          0.0311, -0.0260, -0.0572, -0.0414,  0.0237, -0.0222,  0.0304, -0.0306,\n",
      "          0.0426,  0.0403,  0.0002,  0.0112,  0.0451,  0.0207, -0.0587, -0.0159,\n",
      "         -0.0141,  0.0118,  0.0268, -0.0110, -0.0563, -0.0608, -0.0240, -0.0194,\n",
      "          0.0180, -0.0016,  0.0526,  0.0419,  0.0019,  0.0044, -0.0624, -0.0297,\n",
      "         -0.0208,  0.0086, -0.0212, -0.0040,  0.0534, -0.0347,  0.0092, -0.0474,\n",
      "         -0.0455, -0.0109,  0.0099,  0.0427,  0.0337, -0.0620,  0.0539, -0.0323,\n",
      "          0.0120,  0.0245, -0.0385,  0.0195,  0.0186, -0.0511, -0.0094,  0.0624,\n",
      "          0.0385,  0.0531,  0.0066,  0.0464,  0.0538,  0.0395,  0.0164,  0.0351,\n",
      "          0.0361, -0.0168, -0.0048,  0.0091,  0.0543,  0.0027,  0.0495,  0.0441,\n",
      "         -0.0113, -0.0025, -0.0439,  0.0040, -0.0591, -0.0520, -0.0018,  0.0285,\n",
      "         -0.0527,  0.0605, -0.0272, -0.0335, -0.0603, -0.0415,  0.0590, -0.0117,\n",
      "         -0.0239, -0.0078,  0.0398, -0.0391, -0.0296, -0.0358, -0.0306, -0.0156,\n",
      "         -0.0280,  0.0229, -0.0123,  0.0584, -0.0311,  0.0476,  0.0090,  0.0012,\n",
      "          0.0360, -0.0335, -0.0193, -0.0060,  0.0251,  0.0504, -0.0157, -0.0524,\n",
      "         -0.0012,  0.0383, -0.0493, -0.0114,  0.0411, -0.0315,  0.0581, -0.0524,\n",
      "         -0.0009, -0.0128, -0.0398, -0.0094, -0.0258, -0.0247,  0.0397,  0.0590,\n",
      "         -0.0120, -0.0269, -0.0327, -0.0020, -0.0355,  0.0546, -0.0589, -0.0495,\n",
      "          0.0280, -0.0476,  0.0228, -0.0109,  0.0191, -0.0146, -0.0396,  0.0054,\n",
      "          0.0425, -0.0241,  0.0118, -0.0205,  0.0507, -0.0158, -0.0128, -0.0399,\n",
      "          0.0595, -0.0410, -0.0031, -0.0288,  0.0352, -0.0473,  0.0442,  0.0289,\n",
      "         -0.0203, -0.0477, -0.0330, -0.0484,  0.0499, -0.0105,  0.0580, -0.0090,\n",
      "         -0.0382, -0.0104, -0.0336,  0.0149, -0.0589,  0.0051, -0.0080,  0.0062,\n",
      "         -0.0266, -0.0291,  0.0347,  0.0270,  0.0511,  0.0501, -0.0265,  0.0211,\n",
      "          0.0384,  0.0454, -0.0529, -0.0116, -0.0092, -0.0033,  0.0434,  0.0278,\n",
      "          0.0116, -0.0179,  0.0151, -0.0146, -0.0312, -0.0573,  0.0424,  0.0194,\n",
      "          0.0388,  0.0406, -0.0446,  0.0423, -0.0063,  0.0587, -0.0248, -0.0122,\n",
      "         -0.0345,  0.0593,  0.0166, -0.0484, -0.0144,  0.0324, -0.0310,  0.0108,\n",
      "         -0.0347, -0.0377, -0.0312,  0.0075,  0.0539,  0.0274, -0.0471,  0.0421,\n",
      "          0.0299, -0.0597,  0.0350,  0.0358,  0.0338,  0.0252,  0.0201, -0.0441,\n",
      "          0.0618, -0.0017, -0.0435, -0.0187, -0.0573, -0.0612,  0.0390, -0.0146],\n",
      "        [-0.0270,  0.0326,  0.0121,  0.0190,  0.0544,  0.0053, -0.0212,  0.0445,\n",
      "         -0.0465, -0.0300, -0.0256,  0.0598, -0.0573,  0.0284,  0.0139, -0.0030,\n",
      "         -0.0269, -0.0513,  0.0060,  0.0462,  0.0077, -0.0249, -0.0273, -0.0094,\n",
      "          0.0344,  0.0493, -0.0035,  0.0539,  0.0221,  0.0552,  0.0345,  0.0352,\n",
      "         -0.0079, -0.0532, -0.0137,  0.0125, -0.0363,  0.0127, -0.0236,  0.0452,\n",
      "         -0.0358, -0.0618, -0.0184,  0.0147, -0.0027,  0.0498, -0.0586, -0.0316,\n",
      "         -0.0241,  0.0045, -0.0423, -0.0257, -0.0490,  0.0499, -0.0299, -0.0239,\n",
      "          0.0439,  0.0385, -0.0561, -0.0148, -0.0257,  0.0189, -0.0217, -0.0605,\n",
      "         -0.0068, -0.0439, -0.0580,  0.0393,  0.0209, -0.0030,  0.0609, -0.0391,\n",
      "         -0.0378,  0.0107, -0.0225,  0.0316, -0.0161, -0.0406,  0.0014, -0.0353,\n",
      "          0.0478, -0.0592, -0.0056,  0.0162, -0.0174,  0.0588,  0.0483,  0.0572,\n",
      "         -0.0258, -0.0616, -0.0024,  0.0481,  0.0422,  0.0402,  0.0491, -0.0110,\n",
      "          0.0172,  0.0190,  0.0029, -0.0313, -0.0046, -0.0135,  0.0453,  0.0426,\n",
      "         -0.0582, -0.0528, -0.0208, -0.0035,  0.0546, -0.0153, -0.0261, -0.0498,\n",
      "         -0.0373,  0.0111,  0.0363,  0.0282,  0.0338,  0.0235,  0.0489, -0.0488,\n",
      "         -0.0513,  0.0329,  0.0004, -0.0294,  0.0390,  0.0063, -0.0544, -0.0421,\n",
      "          0.0368, -0.0211,  0.0337, -0.0460,  0.0033,  0.0510, -0.0109,  0.0211,\n",
      "         -0.0095, -0.0604,  0.0356, -0.0061,  0.0513, -0.0542,  0.0227,  0.0477,\n",
      "          0.0498, -0.0180,  0.0500,  0.0522, -0.0235, -0.0058, -0.0451, -0.0387,\n",
      "         -0.0086, -0.0002,  0.0399, -0.0253,  0.0405,  0.0021, -0.0407, -0.0458,\n",
      "         -0.0284,  0.0178, -0.0064, -0.0066, -0.0250, -0.0432, -0.0076, -0.0062,\n",
      "          0.0424, -0.0121, -0.0251,  0.0312,  0.0169, -0.0556,  0.0091, -0.0440,\n",
      "         -0.0372,  0.0057, -0.0610, -0.0181,  0.0177,  0.0356, -0.0102,  0.0319,\n",
      "          0.0319,  0.0185,  0.0579, -0.0063,  0.0019,  0.0004, -0.0106, -0.0484,\n",
      "          0.0462,  0.0137, -0.0103, -0.0203,  0.0121,  0.0444,  0.0229,  0.0008,\n",
      "         -0.0131,  0.0484, -0.0107, -0.0088, -0.0160,  0.0423,  0.0172,  0.0619,\n",
      "          0.0125, -0.0089, -0.0268, -0.0103, -0.0122,  0.0453,  0.0399,  0.0169,\n",
      "         -0.0040, -0.0397, -0.0514,  0.0459,  0.0424,  0.0509, -0.0232, -0.0368,\n",
      "          0.0264, -0.0542,  0.0181,  0.0303, -0.0273, -0.0344, -0.0255,  0.0053,\n",
      "         -0.0297,  0.0004, -0.0332,  0.0499,  0.0241,  0.0207,  0.0489,  0.0492,\n",
      "          0.0315,  0.0313, -0.0133, -0.0267, -0.0619, -0.0286,  0.0116, -0.0249,\n",
      "          0.0176, -0.0556, -0.0583,  0.0389,  0.0305, -0.0085,  0.0213, -0.0091]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: 5.bias | Size: torch.Size([10]) | Values : tensor([0.0528, 0.0529], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model structure: {seq_modules}\\n\\n\")\n",
    "\n",
    "for name, param in seq_modules.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight -- torch.Size([512, 784])\n",
      "1.bias -- torch.Size([512])\n",
      "3.weight -- torch.Size([256, 512])\n",
      "3.bias -- torch.Size([256])\n",
      "5.weight -- torch.Size([10, 256])\n",
      "5.bias -- torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, params in seq_modules.named_parameters():\n",
    "    print(name,'--', params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^\n",
      "torch.Size([256, 512])\n"
     ]
    }
   ],
   "source": [
    "print('^^^^')\n",
    "print(seq_modules[3].weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.weight\n",
      "torch.Size([256, 512])\n",
      "dic torch.Size([256, 512])\n",
      "5.weight\n",
      "torch.Size([10, 256])\n",
      "dic torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "weight_param = {}\n",
    "\n",
    "for idx, (name, params) in enumerate(seq_modules.named_parameters()):\n",
    "    if 'weight' in name:\n",
    "        \n",
    "        if idx == 0:\n",
    "           continue\n",
    "        print(name)\n",
    "        print(params.shape)\n",
    "        weight_param[name] = params\n",
    "        print('dic', weight_param[name].shape )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before reshaping: (256, 512)\n",
      "after reshaping: (512, 256)\n",
      "dmiensional reduction PCA (512, 128)\n",
      "Transpose (128, 512)\n",
      "Assigned value to network weight torch.Size([128, 512])\n",
      "Module Linear(in_features=512, out_features=256, bias=True)\n",
      "----------------------------------------\n",
      "before reshaping: (10, 256)\n",
      "after reshaping: (128, 20)\n",
      "dmiensional reduction PCA (128, 10)\n",
      "Transpose (10, 128)\n",
      "Assigned value to network weight torch.Size([10, 128])\n",
      "Module Linear(in_features=256, out_features=10, bias=True)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for key,value in weight_param.items():\n",
    "\n",
    "    \n",
    "    x = weight_param[key].detach().numpy()\n",
    "    print('before reshaping:', x.shape)\n",
    "\n",
    "    arb = 100   \n",
    "    if x.shape[0] < x.shape[1] and x.shape[0]<=arb:\n",
    "     dim = int(x.shape[1]/2)\n",
    "     x = x.reshape((-1 ,dim))\n",
    "    x = x.T\n",
    "    print('after reshaping:',x.shape)\n",
    "    min_com = min(x.shape[0], x.shape[1])\n",
    "    small_pca = dec.PCA(n_components=int(min_com/2))\n",
    "    small_pca.fit(x)\n",
    "    dr_small  = small_pca.transform(x)\n",
    "    print('dmiensional reduction PCA', dr_small.shape)\n",
    "    dr_small = dr_small.T\n",
    "    print('Transpose', dr_small.shape)\n",
    "\n",
    "    i = key.split('.')\n",
    "    i = int(i[0])\n",
    "\n",
    "\n",
    "    seq_modules[i].weight = nn.Parameter(torch.tensor(dr_small))\n",
    "    # seq_modules[i] = nn.Linear(in_features=dr_small.shape[1], \n",
    "    #                    out_features=dr_small.shape[0], bias=True)\n",
    "\n",
    "    print('Assigned value to network weight',seq_modules[i].weight.shape)\n",
    "    print('Module', seq_modules[i])\n",
    "    print('----------------------------------------')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight torch.Size([512, 784])\n",
      "1.bias torch.Size([512])\n",
      "3.weight torch.Size([128, 512])\n",
      "3.bias torch.Size([256])\n",
      "5.weight torch.Size([10, 128])\n",
      "5.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in seq_modules.named_parameters():\n",
    "    print(name, param.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(seq_modules[3].bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "(64, 4)\n",
      "torch.Size([128])\n",
      "torch.Size([3, 10])\n",
      "tensor([[-0.0134,  0.0868,  0.0486,  0.0409, -0.0349,  0.1044,  0.0408, -0.0313,\n",
      "          0.0672, -0.0822],\n",
      "        [ 0.0342,  0.0885,  0.0799,  0.0814, -0.0293,  0.0430,  0.0711, -0.0375,\n",
      "          0.0579, -0.0517],\n",
      "        [ 0.0633,  0.1137,  0.0433,  0.0168, -0.0337,  0.0327,  0.0352, -0.0210,\n",
      "          0.0520, -0.0577]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(seq_modules.eval())\n",
    "pca = dec.PCA(n_components=2)\n",
    "temp = seq_modules[3].bias.detach().numpy()\n",
    "temp = temp.reshape(64,-1)\n",
    "print(temp.shape)\n",
    "pca.fit(temp)\n",
    "new_bias = pca.transform(temp)\n",
    "new_bias = new_bias.reshape((128))\n",
    "seq_modules[3].bias = nn.Parameter(torch.tensor(new_bias))\n",
    "print(seq_modules[3].bias.shape)\n",
    "\n",
    "result = seq_modules(input_image2)\n",
    "print(result.shape)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f8eb618b7b569c23d3b4b5bc22124c155d58b9880b777276332025b24a4c4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
