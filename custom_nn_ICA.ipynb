{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using different DR technique (ICA) on saved neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\sulta\\anaconda3\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\sulta\\anaconda3\\lib\\site-packages (from scipy) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=5)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading saved network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=12, kernel_size=(5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=(5, 5))\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        # self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN()\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = iter(testloader)\n",
    "images_test, labels_test = test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 8, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "the_output = model(images_test)\n",
    "\n",
    "acc, lb_pred = torch.max(the_output, 1)\n",
    "print(lb_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs_normal = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs_normal.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\n",
    "    f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 400)\n"
     ]
    }
   ],
   "source": [
    "fc1_w = model.fc1.weight.detach().numpy()\n",
    "fc2_w = model.fc2.weight.detach().numpy()\n",
    "fc3_w = model.fc3.weight.detach().numpy()\n",
    "\n",
    "\n",
    "print(fc1_w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc1_ICA = dec.FastICA(n_components=60,\n",
    "            whiten='unit-variance')\n",
    "\n",
    "fc1_reduced = fc1_ICA.fit_transform(fc1_w.T)\n",
    "fc1_reduced  = torch.tensor(fc1_reduced.T ,dtype=torch.float)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 60)\n"
     ]
    }
   ],
   "source": [
    "fc2_ICA = dec.FastICA(n_components=60,\n",
    "            whiten='unit-variance')\n",
    "fc2_reduced = fc2_ICA.fit_transform(fc2_w)\n",
    "print(fc2_reduced.shape)\n",
    "\n",
    "fc2_ICA2 = dec.FastICA(n_components=42, whiten='unit-variance')\n",
    "fc2_reduced = fc2_ICA2.fit_transform(fc2_reduced.T)\n",
    "fc2_reduced = torch.tensor(fc2_reduced.T,  dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "fc3_w = model.fc3.weight.detach().numpy()\n",
    "fc3_w = fc3_w.reshape(fc3_w.shape[0] * 2, -1)\n",
    "fc3_ICA = dec.FastICA(n_components=10, whiten='unit-variance', max_iter=300)\n",
    "fc3_reduced = fc3_ICA.fit_transform(fc3_w.T)\n",
    "fc3_reduced = torch.tensor(fc3_reduced.T, dtype=torch.float)\n",
    "print(fc3_reduced.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 400])\n",
      "torch.Size([42, 60])\n",
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "print(fc1_reduced.shape)\n",
    "print(fc2_reduced.shape)\n",
    "print(fc3_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1 = nn.Linear(400, 60, bias=True)\n",
    "model.fc2 = nn.Linear(60,42,bias=True)\n",
    "model.fc3 = nn.Linear(42,10, bias=True)\n",
    "\n",
    "model.fc1.weight = nn.Parameter(fc1_reduced)\n",
    "model.fc2.weight = nn.Parameter(fc2_reduced)\n",
    "model.fc3.weight = nn.Parameter(fc3_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.6445, 6.9086, 0.7945, 5.2515], grad_fn=<MaxBackward0>)\n",
      "tensor([3, 8, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "the_output = model(images_test)\n",
    "\n",
    "acc, lb_pred = torch.max(the_output, 1)\n",
    "print(acc)\n",
    "print(lb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 62 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs_normal = model(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs_normal.data, 1)\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(\n",
    "    f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.376 acc: 51.750\n",
      "[1,  4000] loss: 1.155 acc: 59.362\n",
      "[1,  6000] loss: 1.134 acc: 60.413\n",
      "[1,  8000] loss: 1.120 acc: 61.112\n",
      "[1, 10000] loss: 1.086 acc: 61.950\n",
      "[1, 12000] loss: 1.097 acc: 62.600\n",
      "epoch: 1\n",
      "[2,  2000] loss: 1.283 acc: 64.640\n",
      "[2,  4000] loss: 1.012 acc: 64.700\n",
      "[2,  6000] loss: 1.005 acc: 65.850\n",
      "[2,  8000] loss: 0.998 acc: 65.188\n",
      "[2, 10000] loss: 1.010 acc: 64.350\n",
      "[2, 12000] loss: 1.023 acc: 64.550\n",
      "epoch: 2\n",
      "[3,  2000] loss: 1.174 acc: 67.530\n",
      "[3,  4000] loss: 0.939 acc: 66.888\n",
      "[3,  6000] loss: 0.949 acc: 67.562\n",
      "[3,  8000] loss: 0.963 acc: 66.875\n",
      "[3, 10000] loss: 0.921 acc: 68.213\n",
      "[3, 12000] loss: 0.963 acc: 66.713\n",
      "epoch: 3\n",
      "[4,  2000] loss: 1.118 acc: 69.110\n",
      "[4,  4000] loss: 0.904 acc: 68.750\n",
      "[4,  6000] loss: 0.933 acc: 68.050\n",
      "[4,  8000] loss: 0.900 acc: 68.700\n",
      "[4, 10000] loss: 0.917 acc: 68.688\n",
      "[4, 12000] loss: 0.914 acc: 68.513\n",
      "epoch: 4\n",
      "[5,  2000] loss: 1.084 acc: 70.520\n",
      "[5,  4000] loss: 0.856 acc: 70.450\n",
      "[5,  6000] loss: 0.873 acc: 69.787\n",
      "[5,  8000] loss: 0.885 acc: 69.588\n",
      "[5, 10000] loss: 0.879 acc: 69.275\n",
      "[5, 12000] loss: 0.906 acc: 68.138\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(optimizer)\n",
    "\n",
    "running_loss = 0.0\n",
    "total = 0.0\n",
    "correct = 0.0\n",
    "loss_nodr =[]\n",
    "acc_nodr = []\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(True)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "             outputs = model(inputs).to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            loss_temp = running_loss / 2000\n",
    "            acc_temp = 100 * correct / total\n",
    "            loss_nodr.append(loss_temp)\n",
    "            acc_nodr.append(acc_temp)\n",
    "            print(\n",
    "                f'[{epoch + 1}, {i + 1:5d}] loss: {loss_temp:.3f} acc: {acc_temp:.3f}')\n",
    "            running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = './model_ICA_reduced.pt'\n",
    "torch.save(model.state_dict, path_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the capacity of the model after applying FastICA onto the weights matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes of data sample:  cat\n",
      "classes of data sample:  ship\n",
      "classes of data sample:  ship\n",
      "classes of data sample:  plane\n"
     ]
    }
   ],
   "source": [
    "data_sample, labels = iter(testloader).next()\n",
    "\n",
    "for l in labels:\n",
    "    print('classes of data sample: ', classes[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([ 0.0106, -0.0324,  0.0332, -0.0228, -0.1344, -0.0133,  0.0336,  0.1185,\n",
      "         0.1332, -0.1631, -0.1097,  0.1063, -0.1316, -0.0879, -0.0189,  0.0023,\n",
      "        -0.1091, -0.0054, -0.0687, -0.2570, -0.1722,  0.0175,  0.0333,  0.0006,\n",
      "         0.1241,  0.0555,  0.1032, -0.0437,  0.2082, -0.0532, -0.0901, -0.2231,\n",
      "         0.0722, -0.0443,  0.3296,  0.0758, -0.3826,  0.0570,  0.0889,  0.1991,\n",
      "         0.1702, -0.0960,  0.0980, -0.2003, -0.1400,  0.0505, -0.1144,  0.0542,\n",
      "         0.0348,  0.2222,  0.0564, -0.0176,  0.2313,  0.0844, -0.1637,  0.1060,\n",
      "        -0.1245,  0.0091,  0.0931,  0.0072], requires_grad=True)\n",
      "(60, 1)\n",
      "net bias updated: torch.Size([60])\n",
      "(42, 1)\n",
      "net bias updated: torch.Size([42])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(model.fc1.bias)\n",
    "bias_n = model.fc1.bias\n",
    "bias_n = bias_n.detach().numpy()\n",
    "bias_n = bias_n.reshape(60, -1)\n",
    "print(bias_n.shape)\n",
    "\n",
    "bias_transformer = dec.FastICA(n_components=1,\n",
    "            random_state=0,\n",
    "            whiten='unit-variance',\n",
    "            tol=2,\n",
    "            max_iter=500)\n",
    "aa = bias_transformer.fit_transform(bias_n)\n",
    "aa = aa.flatten()\n",
    "model.fc1.bias = nn.Parameter(torch.tensor(aa, dtype=torch.float))\n",
    "print('net bias updated:', model.fc1.bias.shape)\n",
    "\n",
    "\n",
    "bias_n = model.fc2.bias\n",
    "bias_n = bias_n.detach().numpy()\n",
    "bias_n = bias_n.reshape(42, -1)\n",
    "print(bias_n.shape)\n",
    "\n",
    "bias_transformer = dec.FastICA(n_components=1,\n",
    "            random_state=0,\n",
    "            whiten='unit-variance',\n",
    "            tol=2,\n",
    "            max_iter=500)\n",
    "aa = bias_transformer.fit_transform(bias_n)\n",
    "aa = aa.flatten()\n",
    "model.fc2.bias = nn.Parameter(torch.tensor(aa, dtype=torch.float))\n",
    "print('net bias updated:', model.fc2.bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor([3, 6, 7, 3])\n",
      "predicted classes:  cat\n",
      "predicted classes:  frog\n",
      "predicted classes:  horse\n",
      "predicted classes:  cat\n"
     ]
    }
   ],
   "source": [
    "result = model(data_sample)\n",
    "print(result.shape)\n",
    "\n",
    "acc, lb_prediction  =  torch.max(result, 1)\n",
    "print(lb_prediction)\n",
    "\n",
    "for lb in lb_prediction:\n",
    "    print('predicted classes: ', classes[lb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f8eb618b7b569c23d3b4b5bc22124c155d58b9880b777276332025b24a4c4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
