{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn.manifold as nonlin\n",
    "import copy\n",
    "import pickle as pk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "#cifar10\n",
    "trainset_cifar = torchvision.datasets.CIFAR10(root='../data/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset_cifar, validset_cifar = torch.utils.data.random_split(trainset_cifar, [45000,5000])\n",
    "\n",
    "\n",
    "\n",
    "trainloader_cifar = torch.utils.data.DataLoader(trainset_cifar, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validloader_cifar = torch.utils.data.DataLoader(validset_cifar, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset_cifar = torchvision.datasets.CIFAR10(root='../data/', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader_cifar = torch.utils.data.DataLoader(testset_cifar, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes_cifar = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# MNIST DATASET\n",
    "mnist_train = torchvision.datasets.MNIST(root='../data/', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "mnist_train, mnist_valid = torch.utils.data.random_split(mnist_train, [50000,10000])\n",
    "mnist_test = torchvision.datasets.MNIST(root='../data/', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader_mnist = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "validloader_mnist =  torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)                                          \n",
    "testloader_mnist = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "classes_mnist =  ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST data sample\n",
    "\n",
    "mnist_dataset_sample = iter(testloader_mnist)\n",
    "sample_im, sample_lbl = next(mnist_dataset_sample)\n",
    "\n",
    "sample_im, sample_lbl = sample_im.to(device), sample_lbl.to(device)\n",
    "\n",
    "\n",
    "## Cifar data sample\n",
    "cifar_dataset_sample = iter(testloader_cifar)\n",
    "sample_im_c, sample_lbl_c = next(cifar_dataset_sample)\n",
    "\n",
    "sample_im_c, sample_lbl_c = sample_im_c.to(device), sample_lbl_c.to(device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functinons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_weights(model,weights):\n",
    "    w1,w2,w3 = weights\n",
    "    model.classifier[0].weight.data = w1\n",
    "    model.classifier[3].weight.data = w2\n",
    "    model.classifier[6].weight.data = w3\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_model(model, layers):\n",
    "    l1, l2, l3 = layers\n",
    "    model.classifier[0] = l1\n",
    "    model.classifier[3] = l2\n",
    "    model.classifier[6] = l3\n",
    "    return model\n",
    "    \n",
    "def change_dimensionality(weight, dr_method):\n",
    "    x1 = weight.detach().cpu().numpy().T\n",
    "    x1 = dr_method.fit_transform(x1)\n",
    "    x1 = torch.tensor(x1.T, dtype=torch.float32).to(device)\n",
    "    print(\"x_new vector\")\n",
    "    print(x1.shape)\n",
    "    return x1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_accuracy(net, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "def every_class_accuracy(model, testloader, classes):\n",
    "    cor_pred = {classname: 0 for classname in classes}\n",
    "    t_pred = {classname: 0 for classname in classes}\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            im, labels = data\n",
    "            output = model(im)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    cor_pred[classes[label]] += 1\n",
    "                t_pred[classes[label]] += 1\n",
    "    \n",
    "    for classname, correct_count in cor_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / t_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def training(model, trainset,valset, n, path):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    loss_nodr = []\n",
    "    acc_nodr = []\n",
    "    \n",
    "    val_loss_nodr = []\n",
    "    model = model.to(device)\n",
    "    val_acc_nodr = []\n",
    "  \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    print(optimizer)\n",
    "\n",
    "    for epoch in range(n):\n",
    "\n",
    "        print('epoch:', epoch)\n",
    "        for i, data in enumerate(trainset):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(True)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "                 \n",
    "                 outputs = model(inputs).to(device)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                running_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val=0.0\n",
    "        correct_val = 0.0\n",
    "\n",
    "        for i, data in enumerate(valset):\n",
    "\n",
    "            inputs, labels2 = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels2 = labels2.to(device)\n",
    "            val_output  = model(inputs).to(device)\n",
    "            loss = criterion(val_output, labels2)\n",
    "            val_loss+=loss.item()\n",
    "            total_val+=labels2.size(0)\n",
    "            _, pred = torch.max(val_output.data, 1)\n",
    "            correct_val += (pred == labels2).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "        train_loss = running_loss/len(trainset)\n",
    "        acc_temp = 100 * correct / total\n",
    "\n",
    "        valid_loss_temp = val_loss/len(valset)\n",
    "        valid_acc_temp = (100 * correct_val)/total_val\n",
    "        loss_nodr.append(train_loss)\n",
    "        acc_nodr.append(acc_temp)\n",
    "\n",
    "        val_loss_nodr.append(valid_loss_temp)\n",
    "        val_acc_nodr.append(valid_acc_temp)\n",
    "        print(\n",
    "            f'[{epoch + 1}, {i + 1:5d}] train loss: {train_loss:.3f} train acc: {acc_temp:.3f}', \n",
    "            f'valid acc: {valid_acc_temp:.3f} valid loss  {valid_loss_temp:.3f} ')\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    pickle_accloss(acc_nodr, loss_nodr,val_acc_nodr,val_loss_nodr,  path)\n",
    "\n",
    "    \n",
    "def pickle_accloss(acc, loss,valid_acc, valid_loss, path):\n",
    "    accandloss = {'accuracy' : acc, 'loss' : loss, 'valid acc' : valid_acc, 'valid loss': valid_loss}\n",
    "    with open(path, 'wb') as ica_acc_and_loss:\n",
    "        pk.dump(accandloss, ica_acc_and_loss)\n",
    "    print('Saved dictionary of loss and accuracy!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "vggmodel = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "vggmodel = vggmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n",
      "[1,  1250] train loss: 0.261 train acc: 91.427 valid acc: 85.600 valid loss  0.456 \n",
      "epoch: 1\n",
      "[2,  1250] train loss: 0.210 train acc: 93.004 valid acc: 86.200 valid loss  0.454 \n",
      "epoch: 2\n",
      "[3,  1250] train loss: 0.163 train acc: 94.740 valid acc: 84.520 valid loss  0.552 \n",
      "Finished Training\n",
      "Saved dictionary of loss and accuracy!\n"
     ]
    }
   ],
   "source": [
    "thepath = './data_vis/cifar10/ica/vgg_model_cifar_vis.pickle'\n",
    "training(vggmodel, trainloader_cifar, validloader_cifar, 3, thepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 84 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(vggmodel, testloader_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_weights(model):\n",
    "    new_weights={}\n",
    "    for i, mod in enumerate(model.classifier):\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            new_weights[i] = mod.weight\n",
    "    return new_weights\n",
    "\n",
    "def drprojection(model,method, whiten):\n",
    "    ret_weights = return_model_weights(model)\n",
    "    weight_list = {}\n",
    "    transform = None\n",
    "    for i,(key,value) in enumerate(ret_weights.items()):\n",
    "        original_weight = value\n",
    "        if transform is not None:\n",
    "            original_weight = torch.mm(value, transform)\n",
    "        temp_list = list(ret_weights)\n",
    "        if i!=len(temp_list)-1:\n",
    "            w_reduced = change_dimensionality(\n",
    "            original_weight, method(n_components=int(value.shape[0]/2),  whiten=whiten, random_state=0, tol= 10)) \n",
    "        else:\n",
    "            w_reduced = original_weight\n",
    "\n",
    "        transform = torch.mm(original_weight, w_reduced.T)\n",
    "        weight_list[key] = w_reduced\n",
    "        \n",
    "    return weight_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_layers(model, layer_type(nn.Conv2D, nn.Linear)): functinon that would reduce specified layer of the model\n",
    "\n",
    "def return_conv_layer_weights(model):\n",
    "    vgg_weight_list = {}\n",
    "    for i,feature in enumerate(model.features):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if isinstance(feature, nn.Conv2d):\n",
    "            print(feature.weight.shape)\n",
    "            vgg_weight_list[i] = feature.weight\n",
    "    return vgg_weight_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conv_reduction(matrix,Dw):\n",
    "    activate = False\n",
    "    matrix = matrix.reshape([matrix.shape[0], matrix.shape[1]*9])\n",
    "    matrix = matrix.T\n",
    "    if Dw is not None:\n",
    "        matrix = matrix.reshape([-1, matrix.shape[1]*9])\n",
    "        matrix = torch.mm(Dw, matrix)\n",
    "        matrix = matrix.reshape([matrix.shape[0]*9, -1])\n",
    "    if matrix.shape[0] < int(matrix.shape[1]/2):\n",
    "        matrix = matrix.reshape([matrix.shape[0]*2, -1])\n",
    "        activate = True\n",
    "    print(matrix.shape)\n",
    "    new_dec = dec.FastICA(n_components=int(matrix.shape[1]/2), whiten='unit-variance', random_state=0, max_iter=350)\n",
    "    reduced_matrix = new_dec.fit_transform(matrix.detach().cpu().numpy())\n",
    "    print(reduced_matrix.shape)\n",
    "    if activate == True:\n",
    "        reduced_matrix = reduced_matrix.reshape([-1, reduced_matrix.shape[1]*2])\n",
    "    reduced_matrix = reduced_matrix.T\n",
    "    t_reduced_matrix = torch.tensor(reduced_matrix, dtype=torch.float32).to(device)\n",
    "    print(t_reduced_matrix.shape)\n",
    "    Dm = torch.mm(t_reduced_matrix, matrix)\n",
    "    print(\"delta matrix: \", Dm.shape)\n",
    "    return (t_reduced_matrix, Dm)\n",
    "\n",
    "def reduced_conv_layer_weights(conv_weight_matrices):\n",
    "    reduced_vgg_params = {}\n",
    "    t_w = None\n",
    "    for key,value in conv_weight_matrices.items():\n",
    "        (reduced_w,Dw) = conv_reduction(value, t_w)\n",
    "        t_w = Dw\n",
    "        reduced_w = reduced_w.reshape((reduced_w.shape[0],reduced_w.shape[1],1,1))\n",
    "        reduced_w = reduced_w.reshape([reduced_w.shape[0],int(reduced_w.shape[1]/9),3,-1])\n",
    "        reduced_vgg_params[key]=reduced_w\n",
    "    return reduced_vgg_params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_reconstruction(vggmodel, weights, layer):\n",
    "#     new_model = copy.deepcopy(vggmodel)\n",
    "#     match layer:\n",
    "#         case 'Linear':   \n",
    "#             for i,w in enumerate(weights):\n",
    "#                 model_layer = nn.Linear(w.shape[1], w.shape[0], False)\n",
    "#                 new_model.classifier[i] = model_layer\n",
    "#                 new_model.classifier[i].weight.data = w\n",
    "#                 return new_model\n",
    "\n",
    "#         case 'Conv':\n",
    "#             for i,w in enumerate(weights):\n",
    "#                 model_layer = nn.Conv2d(w.shape[1], w.shape[0], kernel_size=(w.shape[2], w.shape[3]),\n",
    "#                  stride=(1, 1), padding=(1, 1))\n",
    "#                 new_model.features[i] = model_layer\n",
    "#                 new_model.features[i].weight.data = w\n",
    "#                 return new_model\n",
    "\n",
    "#         case '':\n",
    "#             return 'Please choose layer type(Conv,Linear)'\n",
    "def model_reconstruction(vggmodel, weights, layer):\n",
    "    new_model = copy.deepcopy(vggmodel)\n",
    "    match layer:\n",
    "        case 'Linear':   \n",
    "            for key,value in weights.items():\n",
    "                model_layer = nn.Linear(value.shape[1], value.shape[0], False)\n",
    "                new_model.classifier[key] = model_layer\n",
    "                new_model.classifier[key].weight.data = value\n",
    "            return new_model\n",
    "\n",
    "        case 'Conv':\n",
    "            for key,value in weights.items():\n",
    "                model_layer = nn.Conv2d(value.shape[1], value.shape[0], \n",
    "                kernel_size=(value.shape[2], value.shape[3]),\n",
    "                 stride=(1, 1), padding=(1, 1))\n",
    "                new_model.features[key] = model_layer\n",
    "                new_model.features[key].weight.data = value\n",
    "            return new_model\n",
    "\n",
    "        case '':\n",
    "            return 'Please choose layer type(Conv,Linear)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dr_weight_layer_reduction(model, layertype):\n",
    "    \"\"\"\n",
    "    the functionn that performs dimensional reeduction of weight matrices.\n",
    "    Depending on the layer it would perform dimensional reducton accordingly\n",
    "\n",
    "    :param1: CNN model input p1\n",
    "    :param2: input the type of layer(Conv, Linear) p2\n",
    "    :return: returns a list of dimensioaly reduced matrices\n",
    "    \"\"\" \n",
    "    match layertype: \n",
    "        case 'Conv':\n",
    "            weight_list_vgg = return_conv_layer_weights(vggmodel)\n",
    "            reduced_vgg_conv_weight_list = reduced_conv_layer_weights(weight_list_vgg)\n",
    "            return reduced_vgg_conv_weight_list\n",
    "        case 'Linear':\n",
    "            weight_list = drprojection(vggmodel,dec.FastICA, 'â€˜unit-variance')\n",
    "            return weight_list\n",
    "            \n",
    "        case _:\n",
    "            return 'Provide the layer type like Conv or Linear'\n",
    "\n",
    "\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ica_vggmodel = copy.deepcopy(vggmodel)\n",
    "\n",
    "# vgg_layers = (nn.Linear(25088, 2048, False), nn.Linear(\n",
    "#     2048, 2048, False), nn.Linear(2048, 1000, False))\n",
    "\n",
    "# ica_vggmodel = construct_model(\n",
    "#     ica_vggmodel, vgg_layers)\n",
    "    \n",
    "# ica_vggmodel = are_weights(ica_vggmodel, tuple(weight_list))\n",
    "# ica_vggmodel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_new vector\n",
      "torch.Size([2048, 25088])\n",
      "x_new vector\n",
      "torch.Size([2048, 2048])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([576, 64])\n",
      "(576, 32)\n",
      "torch.Size([32, 576])\n",
      "delta matrix:  torch.Size([32, 64])\n",
      "torch.Size([288, 128])\n",
      "(288, 64)\n",
      "torch.Size([64, 288])\n",
      "delta matrix:  torch.Size([64, 128])\n",
      "torch.Size([576, 128])\n",
      "(576, 64)\n",
      "torch.Size([64, 576])\n",
      "delta matrix:  torch.Size([64, 128])\n",
      "torch.Size([576, 256])\n",
      "(576, 128)\n",
      "torch.Size([128, 576])\n",
      "delta matrix:  torch.Size([128, 256])\n",
      "torch.Size([1152, 256])\n",
      "(1152, 128)\n",
      "torch.Size([128, 1152])\n",
      "delta matrix:  torch.Size([128, 256])\n",
      "torch.Size([1152, 256])\n",
      "(1152, 128)\n",
      "torch.Size([128, 1152])\n",
      "delta matrix:  torch.Size([128, 256])\n",
      "torch.Size([1152, 512])\n",
      "(1152, 256)\n",
      "torch.Size([256, 1152])\n",
      "delta matrix:  torch.Size([256, 512])\n",
      "torch.Size([2304, 512])\n",
      "(2304, 256)\n",
      "torch.Size([256, 2304])\n",
      "delta matrix:  torch.Size([256, 512])\n",
      "torch.Size([2304, 512])\n",
      "(2304, 256)\n",
      "torch.Size([256, 2304])\n",
      "delta matrix:  torch.Size([256, 512])\n",
      "torch.Size([2304, 512])\n",
      "(2304, 256)\n",
      "torch.Size([256, 2304])\n",
      "delta matrix:  torch.Size([256, 512])\n",
      "torch.Size([2304, 512])\n",
      "(2304, 256)\n",
      "torch.Size([256, 2304])\n",
      "delta matrix:  torch.Size([256, 512])\n",
      "torch.Size([2304, 512])\n",
      "(2304, 256)\n",
      "torch.Size([256, 2304])\n",
      "delta matrix:  torch.Size([256, 512])\n"
     ]
    }
   ],
   "source": [
    "linear_weights = dr_weight_layer_reduction(vggmodel, 'Linear')\n",
    "conv_weights = dr_weight_layer_reduction(vggmodel, 'Conv')\n",
    "\n",
    "vgg_reduced_model = model_reconstruction(vggmodel,linear_weights,'Linear')\n",
    "vgg_reduced_model = model_reconstruction(vgg_reduced_model,conv_weights, 'Conv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([32, 64, 3, 3])\n",
      "torch.Size([64, 32, 3, 3])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([2048, 25088])\n",
      "torch.Size([2048, 2048])\n",
      "torch.Size([1000, 2048])\n"
     ]
    }
   ],
   "source": [
    "for conweights in vgg_reduced_model.features:\n",
    "    if isinstance(conweights, nn.Conv2d):\n",
    "        print(conweights.weight.shape)\n",
    "for conweights in vgg_reduced_model.classifier:\n",
    "    if isinstance(conweights, nn.Linear):\n",
    "        print(conweights.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (4x12544 and 25088x2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vgg_reduced_model \u001b[39m=\u001b[39m vgg_reduced_model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_output \u001b[39m=\u001b[39m vgg_reduced_model(sample_im_c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(test_output)\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\vgg.py:69\u001b[0m, in \u001b[0;36mVGG.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[0;32m     68\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier(x)\n\u001b[0;32m     70\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (4x12544 and 25088x2048)"
     ]
    }
   ],
   "source": [
    "vgg_reduced_model = vgg_reduced_model.to(device)\n",
    "test_output = vgg_reduced_model(sample_im_c)\n",
    "print(test_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d3f8c5eab914c59069000d848c6aab31ffabbffb9c6caba6347117ae902c901"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
