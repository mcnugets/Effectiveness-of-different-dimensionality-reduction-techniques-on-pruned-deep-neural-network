{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn.manifold as nonlin\n",
    "import copy\n",
    "import pickle as pk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "#cifar10\n",
    "trainset_cifar = torchvision.datasets.CIFAR10(root='../data/', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainset_cifar, validset_cifar = torch.utils.data.random_split(trainset_cifar, [45000,5000])\n",
    "\n",
    "\n",
    "\n",
    "trainloader_cifar = torch.utils.data.DataLoader(trainset_cifar, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "validloader_cifar = torch.utils.data.DataLoader(validset_cifar, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset_cifar = torchvision.datasets.CIFAR10(root='../data/', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader_cifar = torch.utils.data.DataLoader(testset_cifar, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes_cifar = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# MNIST DATASET\n",
    "mnist_train = torchvision.datasets.MNIST(root='../data/', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "mnist_train, mnist_valid = torch.utils.data.random_split(mnist_train, [50000,10000])\n",
    "mnist_test = torchvision.datasets.MNIST(root='../data/', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader_mnist = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "validloader_mnist =  torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)                                          \n",
    "testloader_mnist = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "classes_mnist =  ('0', '1', '2', '3',\n",
    "           '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functinons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_weights(model,weights):\n",
    "    w1,w2,w3 = weights\n",
    "    model.classifier[0].weight.data = w1\n",
    "    model.classifier[3].weight.data = w2\n",
    "    model.classifier[6].weight.data = w3\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_model(model, layers):\n",
    "    l1, l2, l3 = layers\n",
    "    model.classifier[0] = l1\n",
    "    model.classifier[3] = l2\n",
    "    model.classifier[6] = l3\n",
    "    return model\n",
    "    \n",
    "def change_dimensionality(weight, dr_method):\n",
    "    x1 = weight.detach().cpu().numpy().T\n",
    "    x1 = dr_method.fit_transform(x1)\n",
    "    x1 = torch.tensor(x1.T, dtype=torch.float32).to(device)\n",
    "    print(\"x_new vector\")\n",
    "    print(x1.shape)\n",
    "    return x1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drprojection(model, method, whiten):\n",
    "    weight_list = []\n",
    "    weight1 = model.classifier[0].weight\n",
    "    print('fc1 layer weight')\n",
    "\n",
    "    fc1_reduced = change_dimensionality(\n",
    "        weight1, method(n_components=int(weight1.shape[0]/2),  whiten=whiten, random_state=0, tol= 10))\n",
    "    weight_list.append(fc1_reduced)\n",
    "    x2 = torch.mm(model.classifier[0].weight, fc1_reduced.T)\n",
    "    print(x2.shape)\n",
    "    weight2 = torch.mm(model.classifier[3].weight, x2)\n",
    "    print(weight2.shape)\n",
    "\n",
    "    fc2_reduced = change_dimensionality(\n",
    "        weight2, method(n_components=int(weight2.shape[0]/2),whiten=whiten, random_state=0, tol= 10))\n",
    "    print('fc2 layer weight')\n",
    "    print(fc2_reduced.shape)\n",
    "    weight_list.append(fc2_reduced)\n",
    "\n",
    "    x3_reduced = torch.mm(weight2, fc2_reduced.T)\n",
    "\n",
    "    fc3_reduced = torch.mm(model.classifier[6].weight, x3_reduced)\n",
    "    print('fc3 layer weight')\n",
    "    print(fc3_reduced.shape)\n",
    "    weight_list.append(fc3_reduced)\n",
    "\n",
    "    return weight_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_accuracy(net, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "\n",
    "def every_class_accuracy(model, testloader, classes):\n",
    "    cor_pred = {classname: 0 for classname in classes}\n",
    "    t_pred = {classname: 0 for classname in classes}\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            im, labels = data\n",
    "            output = model(im)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    cor_pred[classes[label]] += 1\n",
    "                t_pred[classes[label]] += 1\n",
    "    \n",
    "    for classname, correct_count in cor_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / t_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def training(model, trainset,valset, n, filename):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    loss_nodr = []\n",
    "    acc_nodr = []\n",
    "    \n",
    "    val_loss_nodr = []\n",
    "    model = model.to(device)\n",
    "    val_acc_nodr = []\n",
    "  \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    print(optimizer)\n",
    "\n",
    "    for epoch in range(n):\n",
    "\n",
    "        print('epoch:', epoch)\n",
    "        for i, data in enumerate(trainset):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(True)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "                 \n",
    "                 outputs = model(inputs).to(device)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                running_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val=0.0\n",
    "        correct_val = 0.0\n",
    "\n",
    "        for i, data in enumerate(valset):\n",
    "\n",
    "            inputs, labels2 = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels2 = labels2.to(device)\n",
    "            val_output  = model(inputs).to(device)\n",
    "            loss = criterion(val_output, labels2)\n",
    "            val_loss+=loss.item()\n",
    "            total_val+=labels2.size(0)\n",
    "            _, pred = torch.max(val_output.data, 1)\n",
    "            correct_val += (pred == labels2).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "        train_loss = running_loss/len(trainset)\n",
    "        acc_temp = 100 * correct / total\n",
    "\n",
    "        valid_loss_temp = val_loss/len(valset)\n",
    "        valid_acc_temp = (100 * correct_val)/total_val\n",
    "        loss_nodr.append(train_loss)\n",
    "        acc_nodr.append(acc_temp)\n",
    "\n",
    "        val_loss_nodr.append(valid_loss_temp)\n",
    "        val_acc_nodr.append(valid_acc_temp)\n",
    "        print(\n",
    "            f'[{epoch + 1}, {i + 1:5d}] train loss: {train_loss:.3f} train acc: {acc_temp:.3f}', \n",
    "            f'valid acc: {valid_acc_temp:.3f} valid loss  {valid_loss_temp:.3f} ')\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    pickle_accloss(acc_nodr, loss_nodr,val_acc_nodr,val_loss_nodr,  filename)\n",
    "\n",
    "    \n",
    "def pickle_accloss(acc, loss,valid_acc, valid_loss, filename):\n",
    "    accandloss = {'accuracy' : acc, 'loss' : loss, 'valid acc' : valid_acc, 'valid loss': valid_loss}\n",
    "    with open('./data_vis/ICA/{}.pickle'.format(filename), 'wb') as ica_acc_and_loss:\n",
    "        pk.dump(accandloss, ica_acc_and_loss)\n",
    "    print('Saved dictionary of loss and accuracy!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "vggmodel = vgg16(weights=VGG16_Weights)\n",
    "vggmodel = vggmodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vggmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n",
      "[1,  1250] train loss: 1.075 train acc: 65.044 valid acc: 78.920 valid loss  0.634 \n",
      "Finished Training\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data_vis/ICA/vgg_cifar10_retrain.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training(vggmodel, trainloader_cifar, validloader_cifar, \u001b[39m1\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mvgg_cifar10_retrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 13\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, trainset, valset, n, filename)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     total \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFinished Training\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m pickle_accloss(acc_nodr, loss_nodr,val_acc_nodr,val_loss_nodr,  filename)\n",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 13\u001b[0m in \u001b[0;36mpickle_accloss\u001b[1;34m(acc, loss, valid_acc, valid_loss, filename)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpickle_accloss\u001b[39m(acc, loss,valid_acc, valid_loss, filename):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=84'>85</a>\u001b[0m     accandloss \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m : acc, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m : loss, \u001b[39m'\u001b[39m\u001b[39mvalid acc\u001b[39m\u001b[39m'\u001b[39m : valid_acc, \u001b[39m'\u001b[39m\u001b[39mvalid loss\u001b[39m\u001b[39m'\u001b[39m: valid_loss}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./data_vis/ICA/\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mformat(filename), \u001b[39m'\u001b[39;49m\u001b[39mwb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m ica_acc_and_loss:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m         pk\u001b[39m.\u001b[39mdump(accandloss, ica_acc_and_loss)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X15sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSaved dictionary of loss and accuracy!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data_vis/ICA/vgg_cifar10_retrain.pickle'"
     ]
    }
   ],
   "source": [
    "training(vggmodel, trainloader_cifar, validloader_cifar, 1, 'vgg_cifar10_retrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 0 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(vggmodel, testloader_cifar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.0\n",
      "Linear(in_features=25088, out_features=4096, bias=True)\n",
      "classifier.3\n",
      "Linear(in_features=4096, out_features=4096, bias=True)\n",
      "classifier.6\n",
      "Linear(in_features=4096, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for a,b in vggmodel.named_modules():\n",
    "    if isinstance(b, nn.Linear):\n",
    "        print(a)\n",
    "        print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_conv_layer_weights(model):\n",
    "    vgg_weight_list = []\n",
    "    for feature in model.features:\n",
    "        if isinstance(feature, nn.Conv2d):\n",
    "            print(feature.weight.shape)\n",
    "            vgg_weight_list.append(feature.weight)\n",
    "    return vgg_weight_list\n",
    "\n",
    "\n",
    "\n",
    "def conv_reduction(matrix):\n",
    "    matrix = matrix.reshape([matrix.shape[0]*3, matrix.shape[1]*3])\n",
    "    matrix = matrix.T\n",
    "    print(matrix.shape)\n",
    "    new_dec = dec.FastICA(n_components=int(matrix.shape[1]/2), whiten='unit-variance', random_state=0)\n",
    "    reduced_matrix = new_dec.fit_transform(matrix.detach().cpu().numpy())\n",
    "    print(reduced_matrix.shape)\n",
    "    reduced_matrix = reduced_matrix.T\n",
    "    reduced_matrix = reduced_matrix.reshape((matrix.shape[0],matrix.shape[1],1,1))\n",
    "    reduced_matrix = reduced_matrix.reshape([int(matrix.shape[0]/3),int(matrix.shape[1]/3),3,-1])\n",
    "    print(reduced_matrix.shape)\n",
    "    return torch.tensor(reduced_matrix, dtype=torch.float32).to(device)\n",
    "\n",
    "def reduced_conv_layer_weights(conv_weight_matrices):\n",
    "    reduced_vgg_params = []\n",
    "    for w in conv_weight_matrices:\n",
    "        r_w = conv_reduction(w)\n",
    "        reduced_vgg_params.append(r_w)\n",
    "    return reduced_vgg_params\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 3, 3])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([9, 192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:550: UserWarning: n_components is too large: it will be set to 9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\decomposition\\_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 81 into shape (9,192,1,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m weight_list_vgg \u001b[39m=\u001b[39m return_conv_layer_weights(vggmodel)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reduced_vgg_conv_weight_list \u001b[39m=\u001b[39m reduced_conv_layer_weights(weight_list_vgg)\n",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 17\u001b[0m in \u001b[0;36mreduced_conv_layer_weights\u001b[1;34m(conv_weight_matrices)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m reduced_vgg_params \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m conv_weight_matrices:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     r_w \u001b[39m=\u001b[39m conv_reduction(w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     reduced_vgg_params\u001b[39m.\u001b[39mappend(r_w)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reduced_vgg_params\n",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\Effectiveness of different dimensionality reduction techniques on pruned deep neural network\\CNN_Pretrained networks\\VGG16\\vgg16_ica.ipynb Cell 17\u001b[0m in \u001b[0;36mconv_reduction\u001b[1;34m(matrix)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m reduced_matrix \u001b[39m=\u001b[39m new_dec\u001b[39m.\u001b[39mfit_transform(matrix\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(reduced_matrix\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m reduced_matrix \u001b[39m=\u001b[39m reduced_matrix\u001b[39m.\u001b[39;49mreshape((matrix\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],matrix\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m],\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m reduced_matrix \u001b[39m=\u001b[39m reduced_matrix\u001b[39m.\u001b[39mreshape([\u001b[39mint\u001b[39m(matrix\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m),\u001b[39mint\u001b[39m(matrix\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m3\u001b[39m),\u001b[39m3\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/Effectiveness%20of%20different%20dimensionality%20reduction%20techniques%20on%20pruned%20deep%20neural%20network/CNN_Pretrained%20networks/VGG16/vgg16_ica.ipynb#X26sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(reduced_matrix\u001b[39m.\u001b[39mshape)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 81 into shape (9,192,1,1)"
     ]
    }
   ],
   "source": [
    "weight_list_vgg = return_conv_layer_weights(vggmodel)\n",
    "reduced_vgg_conv_weight_list = reduced_conv_layer_weights(weight_list_vgg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([2048, 25088])\n",
      "torch.Size([4096, 2048])\n",
      "torch.Size([4096, 2048])\n",
      "x_new vector\n",
      "torch.Size([2048, 2048])\n",
      "fc2 layer weight\n",
      "torch.Size([2048, 2048])\n",
      "fc3 layer weight\n",
      "torch.Size([1000, 2048])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=2048, bias=False)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=2048, out_features=1000, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ica_vggmodel = copy.deepcopy(vggmodel)\n",
    "\n",
    "weight_list = drprojection(vggmodel,dec.FastICA, 'â€˜unit-variance')\n",
    "\n",
    "\n",
    "vgg_layers = (nn.Linear(25088, 2048, False), nn.Linear(\n",
    "    2048, 2048, False), nn.Linear(2048, 1000, False))\n",
    "\n",
    "ica_vggmodel = construct_model(\n",
    "    ica_vggmodel, vgg_layers)\n",
    "    \n",
    "ica_vggmodel = are_weights(ica_vggmodel, tuple(weight_list))\n",
    "ica_vggmodel.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 77 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(ica_vggmodel, testloader_cifar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d3f8c5eab914c59069000d848c6aab31ffabbffb9c6caba6347117ae902c901"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
