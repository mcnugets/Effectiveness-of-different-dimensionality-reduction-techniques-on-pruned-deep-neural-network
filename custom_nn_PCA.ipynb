{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune\n",
    "import sklearn.manifold as nonlin\n",
    "import copy\n",
    "import pickle as pk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "#cifar10\n",
    "trainset_cifar = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                              download=True, transform=transform)\n",
    "trainset_cifar, validset_cifar = torch.utils.data.random_split(trainset_cifar, [\n",
    "                                                               45000, 5000])\n",
    "\n",
    "\n",
    "trainloader_cifar = torch.utils.data.DataLoader(trainset_cifar, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "validloader_cifar = torch.utils.data.DataLoader(validset_cifar, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "\n",
    "testset_cifar = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                             download=True, transform=transform)\n",
    "testloader_cifar = torch.utils.data.DataLoader(testset_cifar, batch_size=batch_size,\n",
    "                                               shuffle=False, num_workers=2)\n",
    "\n",
    "classes_cifar = ('plane', 'car', 'bird', 'cat',\n",
    "                 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "\n",
    "# MNIST DATASET\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                         download=True, transform=transform)\n",
    "mnist_train, mnist_valid = torch.utils.data.random_split(mnist_train, [\n",
    "                                                         50000, 10000])\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader_mnist = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "validloader_mnist = torch.utils.data.DataLoader(mnist_valid, batch_size=batch_size,\n",
    "                                                shuffle=True, num_workers=2)\n",
    "testloader_mnist = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "classes_mnist = ('0', '1', '2', '3',\n",
    "                 '4', '5', '6', '7', '8', '9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_dataset_sample = iter(testloader_mnist)\n",
    "sample_im, sample_lbl = mnist_dataset_sample.next()\n",
    "\n",
    "cifar_dataset_sample = iter(testloader_cifar)\n",
    "sample_im_c, sample_lbl_c = cifar_dataset_sample.next()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function to use later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_weights(model,weights):\n",
    "    w1,w2,w3 = weights\n",
    "    model.fc1.weight.data = w1\n",
    "    model.fc2.weight.data = w2\n",
    "    model.fc3.weight.data = w3\n",
    "    return model\n",
    "\n",
    "\n",
    "def construct_model(model, layers):\n",
    "    l1, l2, l3 = layers\n",
    "    model.fc1 = l1\n",
    "    model.fc2 = l2\n",
    "    model.fc3 = l3\n",
    "    return model\n",
    "    \n",
    "def change_dimensionality(weight, dr_method):\n",
    "    x1 = weight.detach().numpy().T\n",
    "    x1 = dr_method.fit_transform(x1)\n",
    "    x1 = torch.tensor(x1.T, dtype=torch.float32)\n",
    "    print(\"x_new vector\")\n",
    "    print(x1.shape)\n",
    "    return x1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drprojection(model, method, whiten):\n",
    "    weight_list = []\n",
    "    weight1 = model.fc1.weight\n",
    "    print('fc1 layer weight')\n",
    "\n",
    "    fc1_reduced = change_dimensionality(\n",
    "        weight1, method(n_components=int(weight1.shape[0]/2),  whiten=whiten, random_state=0, tol= 10))\n",
    "    weight_list.append(fc1_reduced)\n",
    "    x2 = torch.mm(model.fc1.weight, fc1_reduced.T)\n",
    "    print(x2.shape)\n",
    "    weight2 = torch.mm(model.fc2.weight, x2)\n",
    "    print(weight2.shape)\n",
    "\n",
    "    fc2_reduced = change_dimensionality(\n",
    "        weight2, method(n_components=int(weight2.shape[0]/2),whiten=whiten, random_state=0, tol= 10))\n",
    "    print('fc2 layer weight')\n",
    "    print(fc2_reduced.shape)\n",
    "    weight_list.append(fc2_reduced)\n",
    "\n",
    "    x3_reduced = torch.mm(weight2, fc2_reduced.T)\n",
    "\n",
    "    fc3_reduced = torch.mm(model.fc3.weight, x3_reduced)\n",
    "    print('fc3 layer weight')\n",
    "    print(fc3_reduced.shape)\n",
    "    weight_list.append(fc3_reduced)\n",
    "\n",
    "    return weight_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_accuracy(net, dataset):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            images, labels = data\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\n",
    "        f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_class_accuracy(model, testloader, classes):\n",
    "    cor_pred = {classname: 0 for classname in classes}\n",
    "    t_pred = {classname: 0 for classname in classes}\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            im, labels = data\n",
    "            output = model(im)\n",
    "            _, predictions = torch.max(output, dim=1)\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    cor_pred[classes[label]] += 1\n",
    "                t_pred[classes[label]] += 1\n",
    "    \n",
    "    for classname, correct_count in cor_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / t_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def training(model, trainset,valset, n, filename, is_dr):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    running_loss = 0.0\n",
    "    total = 0.0\n",
    "    correct = 0.0\n",
    "    loss_nodr = []\n",
    "    acc_nodr = []\n",
    "    \n",
    "    val_loss_nodr = []\n",
    "\n",
    "    val_acc_nodr = []\n",
    "  \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    print(optimizer)\n",
    "\n",
    "    for epoch in range(n):\n",
    "\n",
    "        print('epoch:', epoch)\n",
    "        for i, data in enumerate(trainset):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad(True)\n",
    "            with torch.set_grad_enabled(True):\n",
    "                with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "                    if is_dr == True:\n",
    "                        outputs = nn.functional.normalize(model(inputs).to(device))\n",
    "                    else:\n",
    "                        outputs = model(inputs).to(device)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                running_loss += loss.item()\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val=0.0\n",
    "        correct_val = 0.0\n",
    "\n",
    "        for i, data in enumerate(valset):\n",
    "\n",
    "            inputs, labels2 = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels2 = labels2.to(device)\n",
    "            if is_dr == True:\n",
    "                        val_output = nn.functional.normalize(model(inputs).to(device))\n",
    "            else:\n",
    "                        val_output = model(inputs).to(device)\n",
    "            loss = criterion(val_output, labels2)\n",
    "            val_loss+=loss.item()\n",
    "            total_val+=labels2.size(0)\n",
    "            _, pred = torch.max(val_output.data, 1)\n",
    "            correct_val += (pred == labels2).sum().item()\n",
    "\n",
    "            \n",
    "            \n",
    "        train_loss = running_loss/len(trainset)\n",
    "        acc_temp = 100 * correct / total\n",
    "\n",
    "        valid_loss_temp = val_loss/len(valset)\n",
    "        valid_acc_temp = (100 * correct_val)/total_val\n",
    "        loss_nodr.append(train_loss)\n",
    "        acc_nodr.append(acc_temp)\n",
    "\n",
    "        val_loss_nodr.append(valid_loss_temp)\n",
    "        val_acc_nodr.append(valid_acc_temp)\n",
    "        print(\n",
    "            f'[{epoch + 1}, {i + 1:5d}] train loss: {train_loss:.3f} train acc: {acc_temp:.3f}', \n",
    "            f'valid acc: {valid_acc_temp:.3f} valid loss  {valid_loss_temp:.3f} ')\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "    pickle_accloss(acc_nodr, loss_nodr,val_acc_nodr,val_loss_nodr,  filename)\n",
    "\n",
    "    \n",
    "def pickle_accloss(acc, loss,valid_acc, valid_loss, filename):\n",
    "    accandloss = {'accuracy' : acc, 'loss' : loss, 'valid acc' : valid_acc, 'valid loss': valid_loss}\n",
    "    with open('./data_vis/PCA/{}.pickle'.format(filename), 'wb') as ica_acc_and_loss:\n",
    "        pk.dump(accandloss, ica_acc_and_loss)\n",
    "    print('Saved dictionary of loss and accuracy!')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional reduction on Mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, is_normalize):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=12, kernel_size=(5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=(5, 5))\n",
    "        self.fc1 = nn.Linear(256, 120) # 256x 60\n",
    "        self.fc2 = nn.Linear(120, 84) # 60 x 84\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.soft = nn.Softmax(dim=0)\n",
    "        self.norm = is_normalize\n",
    "\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        # self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if(self.norm==True):\n",
    "         x = nn.functional.normalize(self.fc3(x))\n",
    "        else:\n",
    "         x = self.fc3(x)\n",
    "         \n",
    "        # x = self.soft(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TESTING NETWORK WITH DIMEMSNIOALLY REDUCED NETWORK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_original_network_mnist = './pca_reduced_networks/mnist/original_network_mnist.pt'\n",
    "original_network_mnist = torch.load(path_original_network_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 99 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(original_network_mnist, testloader_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: 0     is 99.2 %\n",
      "Accuracy for class: 1     is 99.5 %\n",
      "Accuracy for class: 2     is 99.0 %\n",
      "Accuracy for class: 3     is 98.6 %\n",
      "Accuracy for class: 4     is 98.8 %\n",
      "Accuracy for class: 5     is 99.2 %\n",
      "Accuracy for class: 6     is 98.7 %\n",
      "Accuracy for class: 7     is 99.5 %\n",
      "Accuracy for class: 8     is 99.5 %\n",
      "Accuracy for class: 9     is 98.1 %\n"
     ]
    }
   ],
   "source": [
    "every_class_accuracy(original_network_mnist, testloader_mnist, classes_mnist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dimensional reduction of the original network without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 256])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NN(\n",
       "  (conv1): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(12, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=256, out_features=60, bias=False)\n",
       "  (fc2): Linear(in_features=60, out_features=42, bias=False)\n",
       "  (fc3): Linear(in_features=42, out_features=10, bias=False)\n",
       "  (soft): Softmax(dim=0)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_mnist_network = copy.deepcopy(original_network_mnist)\n",
    "\n",
    "mnist_weight_list = drprojection(\n",
    "    pca_mnist_network, dec.PCA, 'unit-variance')\n",
    "\n",
    "pca_mnist_layers = (nn.Linear(256, 60, False), nn.Linear(\n",
    "    60, 42, False), nn.Linear(42, 10, False))\n",
    "\n",
    "pca_mnist_network = construct_model(\n",
    "    pca_mnist_network, pca_mnist_layers)\n",
    "    \n",
    "pca_mnist_network = are_weights(pca_mnist_network, tuple(mnist_weight_list))\n",
    "pca_mnist_network.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pca_mnist_network, testloader_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dr_mnist_network_untrained = './pca_reduced_networks/mnist/pca_mnist_network.pt'\n",
    "torch.save(pca_mnist_network, path_dr_mnist_network_untrained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pca_mnist_network, testloader_mnist)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retraining dimensionally reduced network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_mnist_network_retrained= copy.deepcopy(pca_mnist_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 49 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pca_mnist_network_retrained, testloader_mnist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2500] train loss: nan train acc: 9.902 valid acc: 9.700 valid loss  nan \n",
      "epoch: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\NN optimization individual project\\custom_nn_PCA.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training(pca_mnist_network_retrained, trainloader_mnist,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m          validloader_mnist, \u001b[39m10\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpca_mnist_retrained_accloss\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\NN optimization individual project\\custom_nn_PCA.ipynb Cell 25\u001b[0m in \u001b[0;36mtraining\u001b[1;34m(model, trainset, valset, n, filename)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m inputs, labels \u001b[39m=\u001b[39m data\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_PCA.ipynb#Y204sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training(pca_mnist_network_retrained, trainloader_mnist,\n",
    "         validloader_mnist, 10, 'pca_mnist_retrained_accloss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pca_mnsit_network_retrained = './pca_reduced_networks/mnist/pca_mnist.network_retrained.pt'\n",
    "torch.save(pca_mnist_network_retrained, path_pca_mnsit_network_retrained)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstructing and training model from scratch without dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_layer_mnist = (nn.Linear(400,60), nn.Linear(60,42), nn.Linear(42,10))\n",
    "small_network_mnist = NN(is_norm=False)\n",
    "small_network_mnist = construct_model(small_network_mnist,small_layer_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(small_network_mnist, testloader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.666 acc: 80.963\n",
      "[1,  4000] loss: 1.486 acc: 95.450\n",
      "[1,  6000] loss: 1.471 acc: 96.062\n",
      "[1,  8000] loss: 1.463 acc: 96.425\n",
      "[1, 10000] loss: 1.456 acc: 97.138\n",
      "[1, 12000] loss: 1.452 acc: 97.338\n",
      "[1, 14000] loss: 1.448 acc: 97.550\n",
      "epoch: 1\n",
      "[2,  2000] loss: 2.168 acc: 97.850\n",
      "[2,  4000] loss: 1.444 acc: 97.825\n",
      "[2,  6000] loss: 1.442 acc: 98.075\n",
      "[2,  8000] loss: 1.444 acc: 97.787\n",
      "[2, 10000] loss: 1.441 acc: 98.162\n",
      "[2, 12000] loss: 1.438 acc: 98.487\n",
      "[2, 14000] loss: 1.442 acc: 98.037\n",
      "epoch: 2\n",
      "[3,  2000] loss: 2.156 acc: 98.450\n",
      "[3,  4000] loss: 1.436 acc: 98.550\n",
      "[3,  6000] loss: 1.435 acc: 98.700\n",
      "[3,  8000] loss: 1.437 acc: 98.475\n",
      "[3, 10000] loss: 1.436 acc: 98.588\n",
      "[3, 12000] loss: 1.435 acc: 98.713\n",
      "[3, 14000] loss: 1.435 acc: 98.562\n",
      "epoch: 3\n",
      "[4,  2000] loss: 2.153 acc: 98.558\n",
      "[4,  4000] loss: 1.433 acc: 98.838\n",
      "[4,  6000] loss: 1.433 acc: 98.737\n",
      "[4,  8000] loss: 1.432 acc: 98.838\n",
      "[4, 10000] loss: 1.432 acc: 98.950\n",
      "[4, 12000] loss: 1.433 acc: 98.688\n",
      "[4, 14000] loss: 1.431 acc: 99.075\n",
      "epoch: 4\n",
      "[5,  2000] loss: 2.148 acc: 98.900\n",
      "[5,  4000] loss: 1.432 acc: 98.900\n",
      "[5,  6000] loss: 1.432 acc: 98.850\n",
      "[5,  8000] loss: 1.430 acc: 99.062\n",
      "[5, 10000] loss: 1.431 acc: 98.950\n",
      "[5, 12000] loss: 1.431 acc: 98.950\n",
      "[5, 14000] loss: 1.432 acc: 98.975\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "training(small_network_mnist, trainloader_mnist, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pca_mnsit_network_scratch = './pca_reduced_networks/mnist/small_network_mnist.pt'\n",
    "torch.save(small_network_mnist, path_pca_mnsit_network_scratch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(small_network_mnist, testloader_mnist)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning mnist net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_function(model, amount_to_prune):\n",
    "    for name,module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            prune.random_unstructured(module, 'weight', amount=amount_to_prune)\n",
    "            print(module._forward_pre_hooks)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(12, <torch.nn.utils.prune.RandomUnstructured object at 0x00000259D73B5640>)])\n",
      "OrderedDict([(13, <torch.nn.utils.prune.RandomUnstructured object at 0x00000259D73B5580>)])\n",
      "OrderedDict([(14, <torch.nn.utils.prune.RandomUnstructured object at 0x00000259D73B5A30>)])\n"
     ]
    }
   ],
   "source": [
    "pruned_mnist_net = copy.deepcopy(original_network)\n",
    "prune_function(pruned_mnist_net, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(original_network, testloader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 74 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pruned_mnist_net, testloader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pruned_net ='./pca_reduced_networks/mnist/pruned/pruned_original_net.pt'\n",
    "torch.save(pruned_mnist_net, path_pruned_net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DR on pruned network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 256])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "weightlist = drprojection(pruned_mnist_net, dec.PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_layers = (nn.Linear(256, 60, False),nn.Linear(60, 42, False), nn.Linear(42, 10, False))\n",
    "pca_pruned_mnist_net = copy.deepcopy(pruned_mnist_net)\n",
    "pca_pruned_mnist_net = construct_model(pca_pruned_mnist_net,pca_layers)\n",
    "pca_pruned_mnist_net = are_weights(pca_pruned_mnist_net, tuple(weightlist))\n",
    "# pca_pruned_mnist_net.fc1.bias.data = biasfc1\n",
    "# pca_pruned_mnist_net.fc2.bias.data = biasfc2\n",
    "# pca_pruned_mnist_net.fc3.bias.data = biasfc3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 25 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pca_pruned_mnist_net, testloader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.max(\n",
      "values=tensor([0.6993, 0.3183, 0.5232, 0.5873], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([9, 3, 1, 9]))\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(pca_pruned_mnist_net(sample_im), dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pruned_pca_net = './pca_reduced_networks/mnist/pruned/pruned_pca_mnist_net.pt'\n",
    "torch.save(pca_pruned_mnist_net, path_pruned_pca_net)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity pca"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " write about how retrieving the bias and dimensioanlly reducing them helps to maintain some infromation however when eliminating the bais off the equation when it comes to network it imporves the overall accuracy of the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i used sparsepca to potentially eliminated the sparse elements of the matrix, hence theoretically should imporve the accuracy in constrats to using PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to stabilise the output of the dimensional reduction and the porjection of the matrices we reconstructed the network structure by normalizing the output, which when compares to original network it can be seen that the cosine similarity is better with normalisation rather than without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.9601, -0.0000, -0.0000,  ...,  3.8460, -0.0000, -0.0000],\n",
      "        [-0.0000,  1.1808, -0.0000,  ..., -3.3882, -0.0000, -0.0000],\n",
      "        [ 0.0000,  2.6545, -0.0000,  ..., -3.3634, -0.0000,  4.7944],\n",
      "        ...,\n",
      "        [-0.0000, -0.7033,  4.4505,  ...,  0.0000, -4.9641,  0.0000],\n",
      "        [ 7.4613,  0.4271,  0.0000,  ...,  2.4404, -0.0000, -0.0000],\n",
      "        [ 0.0000,  0.0000, -5.2224,  ...,  0.8172, -4.9365,  2.2903]])\n",
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 256])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n",
      "tensor([[-0.5485, -4.1330, -5.3164,  ...,  7.1366,  5.6381, -0.2025],\n",
      "        [ 1.6966, -2.1672, -0.2171,  ...,  4.1454, -4.1093, -3.4350],\n",
      "        [ 3.1742,  1.4251,  8.5305,  ..., -1.8840,  1.1888,  1.9020],\n",
      "        ...,\n",
      "        [-0.8448, -5.1814,  4.7656,  ...,  0.7385, -5.5100,  8.2790],\n",
      "        [-3.4957,  1.7812,  5.4462,  ...,  4.7664,  0.9722,  0.8580],\n",
      "        [ 4.0067,  4.4265,  0.8763,  ..., -6.6304,  0.9947,  8.4227]])\n"
     ]
    }
   ],
   "source": [
    "sparsepca_layers = (nn.Linear(256, 60, False),nn.Linear(60, 42, False), nn.Linear(42, 10, False))\n",
    "sparsepca_pruned_mnist_net = copy.deepcopy(pruned_mnist_net)\n",
    "upsacled_weights = []\n",
    "for name, module in sparsepca_pruned_mnist_net.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        upsacled_weights.append(module.weight * 100)\n",
    "\n",
    "sparsepca_pruned_mnist_net = are_weights(sparsepca_pruned_mnist_net, upsacled_weights)\n",
    "sparse_weightlist = drprojection(sparsepca_pruned_mnist_net, dec.MiniBatchSparsePCA)\n",
    "\n",
    "for i in range(len(sparse_weightlist)):\n",
    "    sparse_weightlist[i] =  torch.divide(sparse_weightlist[i], 100)\n",
    "\n",
    "sparsepca_pruned_mnist_net = construct_model(\n",
    "    sparsepca_pruned_mnist_net, sparsepca_layers)\n",
    "sparsepca_pruned_mnist_net = are_weights(sparsepca_pruned_mnist_net, tuple(sparse_weightlist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 34 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(sparsepca_pruned_mnist_net, testloader_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_sparse_pca_mnist_network = './pca_reduced_networks/mnist/pruned/sparse_pca_mnist_network.pt'\n",
    "torch.save(sparsepca_pruned_mnist_net, path_sparse_pca_mnist_network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional reduction on cifar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN2(nn.Module):\n",
    "    def __init__(self, is_norm=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3, out_channels=12, kernel_size=(5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=12, out_channels=16, kernel_size=(5, 5))\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.soft = nn.Softmax(dim=0)\n",
    "        self.is_norm = is_norm\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.pool = nn.MaxPool2d(2, 2)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # self.fc2 = nn.Linear(120, 84)\n",
    "        # self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if(self.is_norm == True):\n",
    "            x = nn.functional.normalize(self.fc3(x))\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_original_net = NN2(is_norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.119 acc: 26.150\n",
      "[1,  4000] loss: 1.991 acc: 38.975\n",
      "[1,  6000] loss: 1.953 acc: 42.850\n",
      "[1,  8000] loss: 1.924 acc: 46.562\n",
      "[1, 10000] loss: 1.908 acc: 47.888\n",
      "[1, 12000] loss: 1.895 acc: 48.625\n",
      "epoch: 1\n",
      "[2,  2000] loss: 2.345 acc: 50.700\n",
      "[2,  4000] loss: 1.858 acc: 53.350\n",
      "[2,  6000] loss: 1.864 acc: 52.388\n",
      "[2,  8000] loss: 1.846 acc: 54.125\n",
      "[2, 10000] loss: 1.851 acc: 53.763\n",
      "[2, 12000] loss: 1.829 acc: 55.688\n",
      "epoch: 2\n",
      "[3,  2000] loss: 2.268 acc: 57.670\n",
      "[3,  4000] loss: 1.809 acc: 58.225\n",
      "[3,  6000] loss: 1.813 acc: 57.650\n",
      "[3,  8000] loss: 1.806 acc: 58.275\n",
      "[3, 10000] loss: 1.810 acc: 58.013\n",
      "[3, 12000] loss: 1.801 acc: 58.938\n",
      "epoch: 3\n",
      "[4,  2000] loss: 2.228 acc: 60.650\n",
      "[4,  4000] loss: 1.782 acc: 61.150\n",
      "[4,  6000] loss: 1.777 acc: 61.263\n",
      "[4,  8000] loss: 1.773 acc: 61.712\n",
      "[4, 10000] loss: 1.779 acc: 61.500\n",
      "[4, 12000] loss: 1.771 acc: 62.050\n",
      "epoch: 4\n",
      "[5,  2000] loss: 2.193 acc: 63.810\n",
      "[5,  4000] loss: 1.755 acc: 63.800\n",
      "[5,  6000] loss: 1.753 acc: 64.188\n",
      "[5,  8000] loss: 1.753 acc: 64.487\n",
      "[5, 10000] loss: 1.757 acc: 63.862\n",
      "[5, 12000] loss: 1.757 acc: 63.413\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "training(cifar_original_net, trainloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_original_network = './pca_reduced_networks/cifar/cifar_original_net.pt'\n",
    "cifar_original_net = torch.load(path_original_network) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 69 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(cifar_original_net, trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 400])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "cifar_weights = drprojection(cifar_original_net, dec.PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers3 = (nn.Linear(400, 60, False), nn.Linear(60, 42, False), nn.Linear(42, 10, False))\n",
    "dr_cifar_untrained_network = construct_model(cifar_original_net, layers3)\n",
    "dr_cifar_untrained_network = are_weights(\n",
    "    dr_cifar_untrained_network, tuple(cifar_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 27 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(dr_cifar_untrained_network, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dr_original_network = './pca_reduced_networks/cifar/cifar_dr_original_net.pt'\n",
    "torch.save(dr_cifar_untrained_network,path_dr_original_network) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dimensioanlly reduced and trained network on cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_cifar_trained_network  = copy.deepcopy(dr_cifar_untrained_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 27 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(dr_cifar_trained_network, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n",
      "[1,  2000] loss: 2.010 acc: 40.188\n",
      "[1,  4000] loss: 2.006 acc: 39.712\n",
      "[1,  6000] loss: 2.000 acc: 41.525\n",
      "[1,  8000] loss: 1.994 acc: 43.175\n",
      "[1, 10000] loss: 1.987 acc: 43.625\n",
      "[1, 12000] loss: 1.976 acc: 44.413\n",
      "epoch: 1\n",
      "[2,  2000] loss: 2.465 acc: 46.540\n",
      "[2,  4000] loss: 1.963 acc: 47.125\n",
      "[2,  6000] loss: 1.964 acc: 47.212\n",
      "[2,  8000] loss: 1.961 acc: 47.312\n",
      "[2, 10000] loss: 1.956 acc: 47.550\n",
      "[2, 12000] loss: 1.949 acc: 48.675\n",
      "epoch: 2\n",
      "[3,  2000] loss: 2.426 acc: 49.990\n",
      "[3,  4000] loss: 1.940 acc: 48.900\n",
      "[3,  6000] loss: 1.935 acc: 50.900\n",
      "[3,  8000] loss: 1.936 acc: 50.688\n",
      "[3, 10000] loss: 1.936 acc: 50.362\n",
      "[3, 12000] loss: 1.927 acc: 51.538\n",
      "epoch: 3\n",
      "[4,  2000] loss: 2.406 acc: 51.290\n",
      "[4,  4000] loss: 1.909 acc: 53.575\n",
      "[4,  6000] loss: 1.917 acc: 53.013\n",
      "[4,  8000] loss: 1.912 acc: 53.575\n",
      "[4, 10000] loss: 1.906 acc: 54.388\n",
      "[4, 12000] loss: 1.910 acc: 53.438\n",
      "epoch: 4\n",
      "[5,  2000] loss: 2.373 acc: 55.320\n",
      "[5,  4000] loss: 1.894 acc: 55.150\n",
      "[5,  6000] loss: 1.901 acc: 54.725\n",
      "[5,  8000] loss: 1.890 acc: 55.425\n",
      "[5, 10000] loss: 1.895 acc: 54.888\n",
      "[5, 12000] loss: 1.887 acc: 55.788\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "training(dr_cifar_trained_network, trainloader, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_net_retrained = 'pca_reduced_networks/cifar/cifar_dr_net_retrained_retrained.pt'\n",
    "torch.save(dr_cifar_trained_network,path_net_retrained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reconstructing model without dimensional redcution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers5 = (nn.Linear(400, 60), nn.Linear(60, 42), nn.Linear(42, 10))\n",
    "reconstruct_network = construct_model(NN2(is_norm=True), layers5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(reconstruct_network, testloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.155 acc: 23.587\n",
      "[1,  4000] loss: 2.021 acc: 36.750\n",
      "[1,  6000] loss: 1.973 acc: 40.562\n",
      "[1,  8000] loss: 1.945 acc: 44.200\n",
      "[1, 10000] loss: 1.931 acc: 44.862\n",
      "[1, 12000] loss: 1.915 acc: 46.337\n",
      "epoch: 1\n",
      "[2,  2000] loss: 2.380 acc: 47.630\n",
      "[2,  4000] loss: 1.892 acc: 48.862\n",
      "[2,  6000] loss: 1.883 acc: 50.087\n",
      "[2,  8000] loss: 1.873 acc: 50.763\n",
      "[2, 10000] loss: 1.857 acc: 52.825\n",
      "[2, 12000] loss: 1.852 acc: 52.675\n",
      "epoch: 2\n",
      "[3,  2000] loss: 2.304 acc: 54.280\n",
      "[3,  4000] loss: 1.838 acc: 55.050\n",
      "[3,  6000] loss: 1.826 acc: 56.312\n",
      "[3,  8000] loss: 1.833 acc: 55.487\n",
      "[3, 10000] loss: 1.824 acc: 56.612\n",
      "[3, 12000] loss: 1.823 acc: 56.750\n",
      "epoch: 3\n",
      "[4,  2000] loss: 2.249 acc: 59.410\n",
      "[4,  4000] loss: 1.806 acc: 58.237\n",
      "[4,  6000] loss: 1.808 acc: 57.950\n",
      "[4,  8000] loss: 1.794 acc: 59.737\n",
      "[4, 10000] loss: 1.796 acc: 59.513\n",
      "[4, 12000] loss: 1.795 acc: 59.300\n",
      "epoch: 4\n",
      "[5,  2000] loss: 2.221 acc: 61.840\n",
      "[5,  4000] loss: 1.782 acc: 61.237\n",
      "[5,  6000] loss: 1.769 acc: 62.625\n",
      "[5,  8000] loss: 1.778 acc: 61.712\n",
      "[5, 10000] loss: 1.775 acc: 62.100\n",
      "[5, 12000] loss: 1.779 acc: 62.212\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "training(reconstruct_network, trainloader, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_net_retrained_scratch = 'pca_reduced_networks/cifar/net_reconstructed_from_scratch.pt'\n",
    "torch.save(reconstruct_network,path_net_retrained_scratch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning on cifar net"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we prune the networkj without the dimensional reduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as cifar dataset is relatively more complex than mnist it is expected to perform more poorly, hence it si expected to have lower accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(9, <torch.nn.utils.prune.RandomUnstructured object at 0x0000013A99639190>)])\n",
      "OrderedDict([(10, <torch.nn.utils.prune.RandomUnstructured object at 0x0000013A90456F40>)])\n",
      "OrderedDict([(11, <torch.nn.utils.prune.RandomUnstructured object at 0x0000013A90477820>)])\n"
     ]
    }
   ],
   "source": [
    "pruned_cifar_net = copy.deepcopy(cifar_original_net)\n",
    "prune_function(pruned_cifar_net, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 39 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(pruned_cifar_net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prune_cifar_net = './pca_reduced_networks/cifar/pruned/pruned_cifar_net.pt'\n",
    "torch.save(pruned_cifar_net,path_prune_cifar_net )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensional reduction on pruned network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pruned_cifar_net = copy.deepcopy(pruned_cifar_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 400])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "pruned_weightlist = drprojection(pca_pruned_cifar_net, dec.PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 26 %\n"
     ]
    }
   ],
   "source": [
    "pca_cifar_layers = (nn.Linear(400, 60,False), nn.Linear(60, 42,False), nn.Linear(42, 10,False))\n",
    "pca_pruned_cifar_net = construct_model(pca_pruned_cifar_net, pca_cifar_layers)\n",
    "pca_pruned_cifar_net = are_weights(pca_pruned_cifar_net, tuple(pruned_weightlist))\n",
    "average_accuracy(pca_pruned_cifar_net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pca_pruned_cifar_net = './pca_reduced_networks/cifar/pruned/pruned_pca_cifar_net.pt'\n",
    "torch.save(pca_pruned_cifar_net, path_pca_pruned_cifar_net)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1 layer weight\n",
      "x_new vector\n",
      "torch.Size([60, 400])\n",
      "torch.Size([120, 60])\n",
      "torch.Size([84, 60])\n",
      "x_new vector\n",
      "torch.Size([42, 60])\n",
      "fc2 layer weight\n",
      "torch.Size([42, 60])\n",
      "fc3 layer weight\n",
      "torch.Size([10, 42])\n"
     ]
    }
   ],
   "source": [
    "sparse_pca_pruned_cifar_net = copy.deepcopy(pruned_cifar_net)\n",
    "\n",
    "sparsepca_layers_cifar = (nn.Linear(400, 60, False), nn.Linear(\n",
    "    60, 42, False), nn.Linear(42, 10, False))\n",
    "upsacled_weights_cifar = []\n",
    "for name, module in sparse_pca_pruned_cifar_net.named_modules():\n",
    "    if isinstance(module, nn.Linear):\n",
    "        upsacled_weights_cifar.append(module.weight * 100)\n",
    "\n",
    "sparse_pca_pruned_cifar_net = are_weights(\n",
    "    sparse_pca_pruned_cifar_net, upsacled_weights_cifar)\n",
    "sparse_weightlist_cifar = drprojection(\n",
    "    sparse_pca_pruned_cifar_net, dec.MiniBatchSparsePCA)\n",
    "\n",
    "for i in range(len(sparse_weightlist_cifar)):\n",
    "    sparse_weightlist_cifar[i] = torch.divide(sparse_weightlist_cifar[i], 100)\n",
    "\n",
    "sparse_pca_pruned_cifar_net = construct_model(\n",
    "    sparse_pca_pruned_cifar_net, sparsepca_layers_cifar)\n",
    "sparse_pca_pruned_cifar_net = are_weights(\n",
    "    sparse_pca_pruned_cifar_net, tuple(sparse_weightlist_cifar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 31 %\n"
     ]
    }
   ],
   "source": [
    "average_accuracy(sparse_pca_pruned_cifar_net, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pruned_sparsepca_cifar_net = './pca_reduced_networks/cifar/pruned/pruned_sparsepca_cifar_net.pt'\n",
    "torch.save(sparse_pca_pruned_cifar_net, path_pruned_sparsepca_cifar_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f8eb618b7b569c23d3b4b5bc22124c155d58b9880b777276332025b24a4c4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
