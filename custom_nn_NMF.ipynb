{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn as sk\n",
    "from sklearn import decomposition as dec\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'NN' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sulta\\Jupyter NoteBooks\\NN optimization individual project\\custom_nn_NMF.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_NMF.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m./cifar_net.pt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_NMF.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModule()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_NMF.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sulta/Jupyter%20NoteBooks/NN%20optimization%20individual%20project/custom_nn_NMF.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[0;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[1;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\serialization.py:1042\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[1;34m(self, mod_name, name)\u001b[0m\n\u001b[0;32m   1040\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m mod_name \u001b[39m=\u001b[39m load_module_mapping\u001b[39m.\u001b[39mget(mod_name, mod_name)\n\u001b[1;32m-> 1042\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'NN' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "path = './cifar_net.pt'\n",
    "model = nn.Module()\n",
    "model = torch.load(path)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Eliminating negative values within the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1, <torch.nn.utils.prune.RandomUnstructured object at 0x000001983DC37D30>)])\n"
     ]
    }
   ],
   "source": [
    "#pruning \n",
    "\n",
    "prune.random_unstructured(model.fc1, name='weight', amount=0.5)\n",
    "print(model.fc1._forward_pre_hooks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1947,  0.0000,  0.0000,  ...,  0.0300,  0.0916, -0.0000],\n",
      "        [ 0.2070, -0.0510, -0.0575,  ...,  0.0000,  0.1574, -0.0447],\n",
      "        [-0.0000, -0.1813, -0.0000,  ..., -0.0422, -0.0474,  0.0000],\n",
      "        ...,\n",
      "        [-0.0000, -0.0000,  0.0016,  ...,  0.0000,  0.0000,  0.0881],\n",
      "        [ 0.0132, -0.0325, -0.0655,  ...,  0.0000, -0.0380,  0.0782],\n",
      "        [-0.2341, -0.3235, -0.0489,  ...,  0.0000,  0.1251,  0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1597, -0.7224, -0.4315,  0.6580,  0.6771,  0.1744, -0.0459, -0.1980,\n",
      "          0.1730, -0.0792],\n",
      "        [-0.1597, -0.7224, -0.4315,  0.6580,  0.6771,  0.1744, -0.0459, -0.1980,\n",
      "          0.1730, -0.0792],\n",
      "        [-0.1597, -0.7224, -0.4315,  0.6580,  0.6771,  0.1744, -0.0459, -0.1980,\n",
      "          0.1730, -0.0792],\n",
      "        [-0.1597, -0.7224, -0.4315,  0.6580,  0.6771,  0.1744, -0.0459, -0.1980,\n",
      "          0.1730, -0.0792]], grad_fn=<AddmmBackward0>)\n",
      "tensor([4, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "datasample = iter(testloader)\n",
    "img, lbl = datasample.next()\n",
    "output = model(img)\n",
    "\n",
    "print(output)\n",
    "acc, pred = torch.max(output, dim=1)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!!\n"
     ]
    }
   ],
   "source": [
    "fc_1 = model.fc1.weight.detach().numpy().T\n",
    "\n",
    "\n",
    "new_nmf = dec.NMF(n_components=60, init='random', random_state=42)\n",
    "fc_1_reduced = new_nmf.fit_transform(fc_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sulta\\anaconda3\\lib\\site-packages\\torch\\autograd\\profiler.py:160: UserWarning: CUDA is not available, disabling CUDA profiling\n",
      "  warn(\"CUDA is not available, disabling CUDA profiling\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.323 acc: 9.925\n",
      "[1,  4000] loss: 2.304 acc: 9.850\n",
      "[1,  6000] loss: 2.303 acc: 10.037\n",
      "[1,  8000] loss: 2.303 acc: 10.137\n",
      "[1, 10000] loss: 2.303 acc: 9.700\n",
      "[1, 12000] loss: 2.303 acc: 10.438\n",
      "epoch: 1\n",
      "[2,  2000] loss: 2.879 acc: 9.820\n",
      "[2,  4000] loss: 2.303 acc: 9.863\n",
      "[2,  6000] loss: 2.303 acc: 9.938\n",
      "[2,  8000] loss: 2.303 acc: 9.963\n",
      "[2, 10000] loss: 2.303 acc: 9.637\n",
      "[2, 12000] loss: 2.303 acc: 10.213\n",
      "epoch: 2\n",
      "[3,  2000] loss: 2.879 acc: 9.830\n",
      "[3,  4000] loss: 2.303 acc: 9.575\n",
      "[3,  6000] loss: 2.303 acc: 10.300\n",
      "[3,  8000] loss: 2.303 acc: 10.537\n",
      "[3, 10000] loss: 2.303 acc: 9.562\n",
      "[3, 12000] loss: 2.303 acc: 9.688\n",
      "epoch: 3\n",
      "[4,  2000] loss: 2.879 acc: 9.730\n",
      "[4,  4000] loss: 2.303 acc: 10.525\n",
      "[4,  6000] loss: 2.303 acc: 9.588\n",
      "[4,  8000] loss: 2.303 acc: 9.637\n",
      "[4, 10000] loss: 2.303 acc: 9.875\n",
      "[4, 12000] loss: 2.303 acc: 9.725\n",
      "epoch: 4\n",
      "[5,  2000] loss: 2.879 acc: 9.800\n",
      "[5,  4000] loss: 2.303 acc: 9.912\n",
      "[5,  6000] loss: 2.303 acc: 10.088\n",
      "[5,  8000] loss: 2.303 acc: 10.075\n",
      "[5, 10000] loss: 2.303 acc: 10.188\n",
      "[5, 12000] loss: 2.304 acc: 9.537\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(optimizer)\n",
    "\n",
    "running_loss = 0.0\n",
    "total = 0.0\n",
    "correct = 0.0\n",
    "loss_nodr =[]\n",
    "acc_nodr = []\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for epoch in range(5):\n",
    "    \n",
    "    print('epoch:', epoch)\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad(True)\n",
    "        with torch.set_grad_enabled(True):\n",
    "            with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n",
    "             outputs = model(inputs).to(device)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            loss_temp = running_loss / 2000\n",
    "            acc_temp = 100 * correct / total\n",
    "            loss_nodr.append(loss_temp)\n",
    "            acc_nodr.append(acc_temp)\n",
    "            print(\n",
    "                f'[{epoch + 1}, {i + 1:5d}] loss: {loss_temp:.3f} acc: {acc_temp:.3f}')\n",
    "            running_loss = 0.0\n",
    "            correct = 0.0\n",
    "            total = 0.0\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f8eb618b7b569c23d3b4b5bc22124c155d58b9880b777276332025b24a4c4f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
